---
layout: post
title: 关于强化学习的一些简单介绍
tags: [machine-learning, reinforcement-learning, sutton-book]
date: 2017-10-03 20:58:00 +08:00
---

## 什么是强化学习

强化学习是非监督学习的一种，我们可以结合下面这个事实例子来理解，它涉及到这么几个子问题：如何做——如何将情境映射到动作——以便最大化数字化的奖励信号。

其实从上面列出的三个子问题，我们可以进一步说明强化学习其实是一个闭环问题，因为学习系统的行为将来又会影响到后面（下一轮学习）的输入

<img src="/assets/images/sub_prob.png" width="40%"/>

此外，学习系统中的“学习者”没有被告知应该采取哪种动作来到达下一状态，而是通过reward的反馈形式来尝试哪些动作或者行为能够让其获得更多的reward。这样的学习机制也反映出强化学习其实是异步的，或者说它的学习过程是有延迟的。它不能够及时的根据当前的境况进行调整，reward的反馈只有在learner完成了一轮学习任务之后才能够被接收，同时反映到下一轮的决策中来。

强化学习相当于是基于过往的学习经验而形成最终的行为的一个学习模型。它的一个特征是在整个学习过程中需要考虑学习和实践的平衡。另一个关键特征是，它明确地描述了一个面向目标的learner与不确定环境交互的整个问题，比如当强化学习应用在游戏中时，玩家的目标很明确，就是比如得到更多的分数。这与许多考虑子问题的方法形成对比。

## 强化学习的几个组成元素

**Policy**

policy定义了学习系统中agent（或者称学习者）在给定时间内的行为方式，粗略地讲，一个policy是一个从预先定义好的环境state集合到动作的映射。它对应于心理学中被称为一组刺激反应规则或关联（刺激包括可能来自动物自身的刺激）。

policy是强化学习中的核心，因为只有这样才能够确定行为。 一般来说，policy可能是随机的。

**Reward**

reward在整体上能够定义一个强化学习问题，因为agent能够通过reward的值的反馈来对周围环境做出一个了解，同时引导agent往其学习目标不断发展优化。比如在一个生物系统中，我们可能将奖励视为类似于快乐或痛苦的经历。

**Value function**

reward直接意义上告诉agent是什么好什么是坏，而值函数则告诉agent在其整个长期的学习中什么是好什么是坏。粗略地讲，当前状态的价值就是从当前这个状态开始以及在后续学习过程中积累的reward的值是多少。

$$V(s) \leftarrow \sum_{i=s}^nR_i$$

**Model**

model起到一个模仿真实环境的作用，或更一般地，可以推断出环境的行为。 例如，给定状态和动作，模型可以预测下一个状态和下一个奖励。 

模型也用于规划，通过这些模型的帮助，能够在实际经历之前考虑未来可能的情况来决定未来的行动过程。使用模型和规划来解决强化学习问题的方法被称为基于模型的方法，而不是简单的无模型的方法，这些方法属于明确的试错学习，被视为几乎与规划相反。 之后会探讨通过反复尝试同时学习的强化学习系统，学习环境模型，并使用该模型进行规划。

## 演化算法和强化学习算法

一些搜索方法，比如genetic algorithms, genetic programming, simulated annealing，以及其他的一些，都曾经被用来解决强化学习问题。他们的共同点是直接在policy空间搜索而不追求价值函数$Q$的体现，我们也把这样的搜索方法称为演化算法，因为这些算法的执行方式可以类比为生物进化。就像生物进化出具有特定行为的器官而不需要它们在有限的生命时间中进行学习。如果policy空间足够小，或者可以结构化，使得好的policy是常见的或容易找到的，那么进化方法就可以有效。此外，进化方法对于学习者无法准确感知其环境状态的问题具有优势。

然而，强化学习的意思是涉及在与环境相互作用的同时进行学习，但这些演化方法不能做。我们认为在许多情况下，能够利用个体行为交互细节的方法比演化算法更有效率。

**演化算法的一些缺点**：演化算法一般都忽视了强化学习问题中的一些有用的结构，比如不考虑它们搜索的那些policy其实是一些关于状态到动作的函数映射，也不注意某个学习个体在其整个学习过程中，动作和状态的选择过程。

## 强化学习的历史发展

强化学习的历史发展分成两条路线，但如今这两条发展线路已经纽结为当今你所看见的强化学习。一个发展方向是通过试错学习（_trail and error_）并且主要应用在动物学习心理学上（_psychology of animal learning_），也正是这样的一些早期研究催生了强化学习。另一个发展方向关注的是优化控制（_optimal control_）并且它采取的解决方案是结合值函数和动态规划。

### 优化控制

优化控制这个概念是在1950末期提出，并用来描述“设计一个能够最小化动态系统测量行为”的问题。其实就是Bellman提出的动态规划理论，Bellman本人也介绍了另一种离散随机版本的优化控制问题，即马尔可夫决策过程（_Markovian decision processes_），然后Ron Howard为MDPs设计了一种决策迭代方法。以上这些奠定了现代强化学习的算法和理论基础。

即使传统的动态规划方法呈现一种计算昂贵，但是相比于同时代的其他方法，这已经是最好的了，所以这一不难理解，在过去的几十年，动态规划在强化学习的很多相关领域都备受青睐。

### 试错学习

我们回到历史发展的另一条线，试错学习。这种方法最早出现在心理学研究中，和学习中的“增强”理论有一些共同点，所以被考虑到强化学习研究中。在心理学研究中，和“增强”理论有对应的是被称为效应法则（_Law of Effect_）的心理学理论。

**选择性（_selectional_）和联想能力（_associative_）**

效应法则包含试错学习利用的两个重要方面，一个是选择性（_selectional_），一个是联想（_associative_）。监督学习具有联想能力，但不具有选择性，而试错学习和效应法则包含这两种特性的结合。我们可以这样理解：试错学习是一种简单的搜索和记忆方法，通过选择和尝试的方式进行搜索和记忆，以记忆哪些行为最有效的方式进行记忆，然后将他们联系起来。

Minsky在他的博士论文中提出了一种强化学习计算模型，并描述了他构建的模拟机器，他称之为SNARCs（随机神经 - 模拟增强计算器，_Stochastic Neural-Analog Reinforcement Calculators_）组成，这也是强化学习模型的第一次出现。之后Farley和Clark描述了另一种被设计用来进行试错学习的神经网络学习机器，而有趣的是，这两人在后来又从试错学习的研究转向泛化研究和模式识别问题中去了，也就是从强化学习阵营转向监督学习。这样的研究方向转变竟然导致这些研究话题之间的关系模糊化，很多研究者似乎开始相信他们在进行的强化学习研究工作实际的落脚点在监督学习。也正因为这样的意识上的模糊，导致在1960-1970年代之间实际关于试错学习的研究非常少。

在1960-1970年之间，有显著成果的应该算是John Andreae和Donald Michie。前者在1963年提出STeLLA系统，一个通过试错学习与环境进行交互的系统，该系统包含一个内部世界模型以及一个处理隐藏状态问题的agent；后者作出的工作更有影响力，在1961和1963年，他描述了一个称为MENACE的用来完成tic-tac-toe游戏的简单试错学习系统。

<img src="/assets/images/tic_toe.png" width="40%"/>

### 时间差异学习（_temporal-difference learning_）

Temporal-difference learning methods are distinctive in being driven by the difference between temporally successive estimates of the same quantity—for example, of the probability of winning in the tic-tac-toe example. This thread is smaller and less distinct than the other two, but it has played a particularly important role in the field, in part because temporal-difference methods seem to be new and unique to reinforcement learning.

