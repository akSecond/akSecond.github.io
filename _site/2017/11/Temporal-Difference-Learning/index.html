<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <title>Temporal-Difference Learning</title>
  <meta name="description" content="TD learning是对Monte Carlo和DP算法的综合，像Monte Carlo一样属于一种Modle Free算法，即不需要对环境模型有一个具体的认识（比如需要知道状态转移的Markov决策过程）；同时也像DP一样属于一种迭代更新算法。也许这个表达式可以更容易地帮助理解：">
  <meta name="author" content="Wei Wang">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Temporal-Difference Learning">
  <meta name="twitter:description" content="TD learning是对Monte Carlo和DP算法的综合，像Monte Carlo一样属于一种Modle Free算法，即不需要对环境模型有一个具体的认识（比如需要知道状态转移的Markov决策过程）；同时也像DP一样属于一种迭代更新算法。也许这个表达式可以更容易地帮助理解：">
  
  <meta property="og:type" content="article">
  <meta property="og:title" content="Temporal-Difference Learning">
  <meta property="og:description" content="TD learning是对Monte Carlo和DP算法的综合，像Monte Carlo一样属于一种Modle Free算法，即不需要对环境模型有一个具体的认识（比如需要知道状态转移的Markov决策过程）；同时也像DP一样属于一种迭代更新算法。也许这个表达式可以更容易地帮助理解：">
  
  <link rel="icon" type="image/png" href="/assets/images/favicon.png" />
  <link href="/assets/images/favicon.png" rel="shortcut icon" type="image/png">
  
  <link rel="stylesheet" href="/css/main.css">
  <link href="//netdna.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">

  <link rel="canonical" href="http://kornbergfresnel.github.io/2017/11/Temporal-Difference-Learning/">
  <link rel="alternate" type="application/rss+xml" title="Andrew Kornberg" href="http://kornbergfresnel.github.io/feed.xml">
  
  <meta name="google-site-verification" content="1-1ZlHoRvM0T2FqPbW2S-qLgYXN6rsn52kErlMPd_gw" />

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>  
  
</head>


  <body>

    <span class="mobile btn-mobile-menu">
        <i class="fa fa-list btn-mobile-menu__icon"></i>
        <i class="fa fa-angle-up btn-mobile-close__icon hidden"></i>
    </span>
    
    <header class="panel-cover panel-cover--collapsed" style="background-image: url('/assets/images/background-cover.jpg')">
  <div class="panel-main">

    <div class="panel-main__inner panel-inverted">
    <div class="panel-main__content">

        <a href="/#blog" title="前往 Andrew Kornberg 的主页" class="blog-button"><img src="/assets/images/avatar.jpg" width="80" alt="Andrew Kornberg logo" class="panel-cover__logo logo" /></a>
        <h1 class="panel-cover__title panel-title"><a href="/#blog" title="link to homepage for Andrew Kornberg" class="blog-button">Andrew Kornberg</a></h1>
        
        <span class="panel-cover__subtitle panel-subtitle">abeunt studia in morse</span>
        
        <hr class="panel-cover__divider" />
        <p class="panel-cover__description">Hi, I'm Kornberg, a wild programmer & PhD. candidate</p>
        <hr class="panel-cover__divider panel-cover__divider--secondary" />
        
        
        
        <div class="navigation-wrapper">
          <div>
            <nav class="cover-navigation cover-navigation--primary">
              <ul class="navigation">
                <li class="navigation__item"><a href="/#blog" title="Visit blog" class="blog-button">Blog</a></li>
                
                  <li class="navigation__item"><a href="http://kornbergfresnel.github.io" target="_blank" title="My open-source projects">Projects</a></li>
                
              </ul>
            </nav>
          </div>
          
          <div><nav class="cover-navigation navigation--social">
  <ul class="navigation">

  
  <!-- Weibo -->
  <li class="navigation__item">
    <a href="http://weibo.com/u/5104230793" title="@u/5104230793 的微博" target="_blank">
      <i class='social fa fa-weibo'></i>
      <span class="label">Weibo</span>
    </a>
  </li>
  

  
  <!-- Github -->
  <li class="navigation__item">
    <a href="https://github.com/KornbergFresnel" title="@KornbergFresnel 的 Github" target="_blank">
      <i class='social fa fa-github'></i>
      <span class="label">Github</span>
    </a>
  </li>
  
  
  
  <!-- Twitter -->
  <li class="navigation__item">
    <a href="http://twitter.com/andrew_kornberg" title="@andrew_kornberg" target="_blank">
      <i class='social fa fa-twitter'></i>
      <span class="label">Twitter</span>
    </a>
  </li>
  

  
  <!-- Google Plus -->
  <li class="navigation__item">
    <a href="https://plus.google.com/107108267983477358170" rel="author" title="Google+" target="_blank">
      <i class='social fa fa-google-plus-square'></i>
      <span class="label">Google Plus</span>
    </a>
  </li>
  

  <!-- RSS -->
  <li class="navigation__item">
    <a href="/feed.xml" rel="author" title="RSS" target="_blank">
      <i class='social fa fa-rss'></i>
      <span class="label">RSS</span>
    </a>
  </li>

  
  <!-- Email -->
  <li class="navigation__item">
    <a href="mailto:kornbergfresnel@outlook.com" title="Contact me">
      <i class='social fa fa-envelope'></i>
      <span class="label">Email</span>
    </a>
  </li>
  

  </ul>
</nav>
</div>
        </div>
      </div>
    </div>
    
    
    <div class="panel-cover--overlay cover-blue"></div>
    
  </div>
</header>


    <div class="content-wrapper">
        <div class="content-wrapper__inner">
            

<article class="post-container post-container--single" itemscope itemtype="http://schema.org/BlogPosting">
  <header class="post-header">
    <div class="post-meta">
      <time datetime="2017-11-03 02:00:00 +0800" itemprop="datePublished" class="post-meta__date date">2017-11-03</time> &#8226; <span class="post-meta__tags tags"><i class="fa fa-tags"></i> <a href="/tags/machine-learning/">Machine Learning</a>, <a href="/tags/reinforcement-learning/">Reinforcement Learning</a>, <a href="/tags/sutton-book/">Sutton Book</a></span>
    </div>
    <h1 class="post-title">Temporal-Difference Learning</h1>
  </header>

  <section class="post">
    <p>TD learning是对Monte Carlo和DP算法的综合，像Monte Carlo一样属于一种Modle Free算法，即不需要对环境模型有一个具体的认识（比如需要知道状态转移的Markov决策过程）；同时也像DP一样属于一种迭代更新算法。也许这个表达式可以更容易地帮助理解：</p>

<script type="math/tex; mode=display">V(S_t) \leftarrow V(S_t) + \alpha[R+\gamma V(S_{t+1})-V(S_t)]</script>

<p>其中 $V(S_t)$ 表示在 $t$ 时刻状态为 $S$ 的 state value，在Monte Carlo里面，$V(S)$ 的估计通常采用 <em>first visit</em> 进行，以保证估计过程的 <em>unbias</em> 特性。从公式角度来看，和Monte Carlo唯一的区别就是把原来的 $G_t$ 改成了基于下一时刻 state value 的估计 $V(S_{t+1})$ 得到的 $V’(S_t)$。下面是对TD(0)的算法描述：</p>

<p><img src="/assets/images/td0.png" width="50%" /></p>

<p>这里有这么一个问题，虽然从递归的性质来看，逻辑关系没有错误，但是所有的 $V(S_t)$ 的预测都是基于对 $V(S_{t+1})$ 预测的预测，假设对 $V(S_{t+1})$ 的预测有bias，怎么就能保证 $V(S_t)$ 的预测的偏差不会出现累积效应，或者说越来越偏离真实值？</p>

<p><strong>TD算法的优点</strong></p>

<p>TD算法和DP以及Monte Carlo相比，首先的优点就是计算空间小，达到optimal的时间短，以及model-free。尤其是在有些长episode场景中，由于Monte Carlo需要在每个episode结束的时候才开始进行state value或者action value的更新，那么达到收敛的时间花费显然要高，相比TD的step by step更新就很cost。</p>

<p>当然到这里为止还是没有解决上面那个问题，实际试验结果显示TD确实能够像Monte Carlo那样收敛到最优值，但没有给出实际的数学证明（所以这里先埋个坑，过段时间再来填）</p>

<p><strong>TD(0)的最优性</strong></p>

<p>在学习率 $\alpha$ 的值足够小的情况下，batch TD能够收敛一个最优值，同时不依赖于 $\alpha$ 的取值，在相同的条件下，Monte Carlo也可以收敛到一个最优值，当然这两个值不一定相同。因为两者的优化目标是不一样的，Monte Carlo通过做出和训练集相比均方误差最小的值估计来逼近最优，而TD算法是通过对Markov决策的最大似然估计来逼近最优解。举个例子：这里有8组episode的抽样模拟结果：</p>

<p><img src="/assets/images/example64.png" width="50%" /></p>

<p>接下来分别使用TD和Monte Carlo来对两种状态的 $V$ 进行估计，显然 $V(B)$ 的值很好算，在两种算法的情况下都是 $3 \over 4$，而对状态 $A$ 的state value估计却有两种情况，在Monte Carlo里面，$V(A)=0$，在TD里面 $V(A)={3 \over 4}$，然而实际的状态转移情况是：</p>

<p><img src="/assets/images/example642.png" width="50%" /></p>

<p>两种估值都没有错，因为都能够控制从A走到B，Monte Carlo保证了在训练集上的error最小（实际上这里没有误差），而TD虽然在训练集上的误差要比Monte Carlo大，但是在将来的情况表现得会比Monte Carlo的预测要好。</p>

<p>这里接入有关TD(0)的几种算法</p>

<p><strong>Sarsa：On-policy TD(0) control</strong></p>

<p>Sarsa在保证epsiode的时间有限的情况下，能够以1.0的概率收敛到最优的policy和action value</p>

<p><img src="/assets/images/sarsa_td.png" width="50%" /></p>

<p><strong>Q-learning：Off-policy TD(0) control</strong></p>

<p><img src="/assets/images/q_learning_td.png" width="50%" /></p>

<p>Q-learning和Sarsa的不同点可以这样来理解，前者总是通过选取action value最大的动作来执行，后者通过可能通过一个更安全的动作来执行，因为每次动作的选取都是基于policy的。</p>

<p><strong>Expected Sarsa</strong></p>

<p>和普通的Sarsa相比，该算法的特点是弱化了由于policy的random特性而导致的方差过大，同时计算量也上升了</p>

<script type="math/tex; mode=display">Q(S_t, A_t) \leftarrow Q(S_t,A_t)+\alpha[R_{t+1}+\gamma E[Q(S_{t+1},A_{t+1}) \mid S_{t+1}]-Q(S_t,A_t)]</script>

<script type="math/tex; mode=display">Q(S_t, A_t) \leftarrow Q(S_t,A_t)+\alpha[R_{t+1}+\gamma \sum_a{\pi(a\mid S_{t+1})Q(S_{t+1},A_{t+1})}-Q(S_t,A_t)]</script>

<p>一般的Q-learning有一个缺点，假设现在状态 $S$ 的最优action value真实值是0，而它目前的所有action估计值有正有负有零，从其预测更新的规则来看，每次都是选取最大的预测值，那么这样一来就会导致整体出现一个正向bias。有一种解决方法，通过将动作预测和执行分开来做，能够保证这个过程是无偏估计。</p>

<p><strong>Double Q-learning</strong></p>

<p>假设现在有两套policy的参数，一套 $Q_1$ 用来进行action预测，一套 $Q_2$ 用来进行动作的执行，也就是进行当前state的action value更新，那么假设用来更新的动作 $A^*=\arg \max_a Q_1(a)$，那么</p>

<script type="math/tex; mode=display">Q(s,a)=Q(s,a)+\alpha[R+\gamma Q_2(s',\arg \max_aQ_1(s',a))-Q(s,a)]</script>

<p>还要说明一下的是其中一个 $Q$ 是作为target来考虑的，也就是其中一个被当作最新的policy，用来产生 $A^*$ 的那套参数便是最新的。那么可以这样来理解，我用最新的一套policy产生了一个action，而这个action由于迭代关系又是与上一次的policy有关的，也就相当于满足老的policy的分布，而这个分布暂时还在另一个Q里面，所以现在把这个action拿给另一个Q来预测更新就是无偏的。</p>

<p><img src="/assets/images/double_q.png" width="50%" /></p>

  </section>

  <div class="post-meta">
      <span class="post-meta__tags tags"><i class="fa fa-tags"></i> <a href="/tags/machine-learning/">Machine Learning</a>, <a href="/tags/reinforcement-learning/">Reinforcement Learning</a>, <a href="/tags/sutton-book/">Sutton Book</a></span>
    </div>
</article>

<section class="read-more">
   
   
   <div class="read-more-item">
       <span class="read-more-item-dim">最近的文章</span>
       <h2 class="post-list__post-title post-title"><a href="/2017/11/Prove-of-Double-Q-learning/" title="link to Prove of Double Q-learning">Prove of Double Q-learning</a></h2>
       <p class="excerpt">In some stochastic environments the well-known reinforcement learning algorithm Q-learning performs very poorly. This poor performance is caused by large overestimations of action values. These overestimations result from a positive bias that is i...&hellip;</p>
       <div class="post-list__meta"><time datetime="2017-11-14 00:20:00 +0800" class="post-list__meta--date date">2017-11-14</time> <a class="btn-border-small" href=/2017/11/Prove-of-Double-Q-learning/>继续阅读</a></div>
   </div>
   
   
   
   
   <div class="read-more-item">
       <span class="read-more-item-dim">更早的文章</span>
       <h2 class="post-list__post-title post-title"><a href="/2017/11/SILENCE/" title="link to SILENCE">SILENCE</a></h2>
       <p class="excerpt">​​Yeah, I'd rather be a lover than a fighter​Cause all my life, I've been fighting​Never felt a feeling of comfort​All this time, I've been hiding ​And I never had someone to call my own, oh nah​I'm so used to sharing​Love only left me alone​But I...&hellip;</p>
       <div class="post-list__meta"><time datetime="2017-11-02 23:30:00 +0800" class="post-list__meta--date date">2017-11-02</time><a class="btn-border-small" href=/2017/11/SILENCE/>继续阅读</a></div>
   </div>
   
</section>

<section class="post-comments">
  
    <div id="disqus_thread"></div>
    <script>
    
    var disqus_config = function () {
        this.page.url = "http://kornbergfresnel.github.io/2017/11/Temporal-Difference-Learning/";
        this.page.identifier = "/2017/11/Temporal-Difference-Learning/";
    };

    var disqus_shortname = 'vno-jekyll';
    
    (function() { // DON'T EDIT BELOW THIS LINE
        var d = document, s = d.createElement('script');
        s.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();
    </script>
    <noscript>要查看<a href="http://disqus.com/?ref_noscript"> Disqus </a>评论，请启用 JavaScript</noscript>
    
  
  
  
  
</section>


            <section class="footer">
    <footer>
    	<span class="footer__copyright">本站点采用<a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">知识共享 署名-非商业性使用-相同方式共享 4.0 国际 许可协议</a></span>
        <span class="footer__copyright">由 <a href="https://jekyllrb.com">Jekyll</a> 于 2018-01-06 生成，感谢 <a href="https://www.digitalocean.com/?refcode=30ed2d146762">Digital Ocean</a> 为本站提供稳定的 VPS 服务</span>
        <span class="footer__copyright">本站由 <a href="http://twitter/andrew_kornberg">@andrew_kornberg</a> 创建，采用 <a href="https://github.com/onevcat/vno-jekyll">Vno - Jekyll</a> 作为主题，您可以在 GitHub 找到<a href="https://github.com/onevcat/OneV-s-Den">本站源码</a> - &copy; 2018</span>
    </footer>
</section>

        </div>
    </div>
    
    <script type="text/javascript" src="//code.jquery.com/jquery-1.11.3.min.js"></script>

<script type="text/javascript" src="/js/main.js"></script>



    
  </body>

</html>
