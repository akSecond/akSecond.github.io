<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <title>Prove of Double Q-learning</title>
  <meta name="description" content="In some stochastic environments the well-known reinforcement learning algorithm Q-learning performs very poorly. This poor performance is caused by large ove...">
  <meta name="author" content="Wei Wang">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Prove of Double Q-learning">
  <meta name="twitter:description" content="In some stochastic environments the well-known reinforcement learning algorithm Q-learning performs very poorly. This poor performance is caused by large ove...">
  
  <meta property="og:type" content="article">
  <meta property="og:title" content="Prove of Double Q-learning">
  <meta property="og:description" content="In some stochastic environments the well-known reinforcement learning algorithm Q-learning performs very poorly. This poor performance is caused by large ove...">
  
  <link rel="icon" type="image/png" href="/assets/images/favicon.png" />
  <link href="/assets/images/favicon.png" rel="shortcut icon" type="image/png">
  
  <link rel="stylesheet" href="/css/main.css">
  <link href="//netdna.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">

  <link rel="canonical" href="http://kornbergfresnel.github.io/2017/11/Prove-of-Double-Q-learning/">
  <link rel="alternate" type="application/rss+xml" title="Andrew Kornberg" href="http://kornbergfresnel.github.io/feed.xml">
  
  <meta name="google-site-verification" content="1-1ZlHoRvM0T2FqPbW2S-qLgYXN6rsn52kErlMPd_gw" />

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>  
  
</head>


  <body>

    <span class="mobile btn-mobile-menu">
        <i class="fa fa-list btn-mobile-menu__icon"></i>
        <i class="fa fa-angle-up btn-mobile-close__icon hidden"></i>
    </span>
    
    <header class="panel-cover panel-cover--collapsed" style="background-image: url('/assets/images/background-cover.jpg')">
  <div class="panel-main">

    <div class="panel-main__inner panel-inverted">
    <div class="panel-main__content">

        <a href="/#blog" title="前往 Andrew Kornberg 的主页" class="blog-button"><img src="/assets/images/avatar.jpg" width="80" alt="Andrew Kornberg logo" class="panel-cover__logo logo" /></a>
        <h1 class="panel-cover__title panel-title"><a href="/#blog" title="link to homepage for Andrew Kornberg" class="blog-button">Andrew Kornberg</a></h1>
        
        <span class="panel-cover__subtitle panel-subtitle">abeunt studia in morse</span>
        
        <hr class="panel-cover__divider" />
        <p class="panel-cover__description">Hi, I'm Kornberg, a wild programmer & PhD. candidate</p>
        <hr class="panel-cover__divider panel-cover__divider--secondary" />
        
        
        
        <div class="navigation-wrapper">
          <div>
            <nav class="cover-navigation cover-navigation--primary">
              <ul class="navigation">
                <li class="navigation__item"><a href="/#blog" title="Visit blog" class="blog-button">Blog</a></li>
                
                  <li class="navigation__item"><a href="http://kornbergfresnel.github.io" target="_blank" title="My open-source projects">Projects</a></li>
                
              </ul>
            </nav>
          </div>
          
          <div><nav class="cover-navigation navigation--social">
  <ul class="navigation">

  
  <!-- Weibo -->
  <li class="navigation__item">
    <a href="http://weibo.com/u/5104230793" title="@u/5104230793 的微博" target="_blank">
      <i class='social fa fa-weibo'></i>
      <span class="label">Weibo</span>
    </a>
  </li>
  

  
  <!-- Github -->
  <li class="navigation__item">
    <a href="https://github.com/KornbergFresnel" title="@KornbergFresnel 的 Github" target="_blank">
      <i class='social fa fa-github'></i>
      <span class="label">Github</span>
    </a>
  </li>
  
  
  
  <!-- Twitter -->
  <li class="navigation__item">
    <a href="http://twitter.com/andrew_kornberg" title="@andrew_kornberg" target="_blank">
      <i class='social fa fa-twitter'></i>
      <span class="label">Twitter</span>
    </a>
  </li>
  

  
  <!-- Google Plus -->
  <li class="navigation__item">
    <a href="https://plus.google.com/107108267983477358170" rel="author" title="Google+" target="_blank">
      <i class='social fa fa-google-plus-square'></i>
      <span class="label">Google Plus</span>
    </a>
  </li>
  

  <!-- RSS -->
  <li class="navigation__item">
    <a href="/feed.xml" rel="author" title="RSS" target="_blank">
      <i class='social fa fa-rss'></i>
      <span class="label">RSS</span>
    </a>
  </li>

  
  <!-- Email -->
  <li class="navigation__item">
    <a href="mailto:kornbergfresnel@outlook.com" title="Contact me">
      <i class='social fa fa-envelope'></i>
      <span class="label">Email</span>
    </a>
  </li>
  

  </ul>
</nav>
</div>
        </div>
      </div>
    </div>
    
    
    <div class="panel-cover--overlay cover-blue"></div>
    
  </div>
</header>


    <div class="content-wrapper">
        <div class="content-wrapper__inner">
            

<article class="post-container post-container--single" itemscope itemtype="http://schema.org/BlogPosting">
  <header class="post-header">
    <div class="post-meta">
      <time datetime="2017-11-14 00:20:00 +0800" itemprop="datePublished" class="post-meta__date date">2017-11-14</time> &#8226; <span class="post-meta__tags tags"><i class="fa fa-tags"></i> <a href="/tags/machine-learning/">Machine Learning</a>, <a href="/tags/reinforcement-learning/">Reinforcement Learning</a></span>
    </div>
    <h1 class="post-title">Prove of Double Q-learning</h1>
  </header>

  <section class="post">
    <p>In some stochastic environments the well-known reinforcement learning algorithm Q-learning performs very poorly. This poor performance is caused by large overestimations of action values. These overestimations result from a positive bias that is introduced because Q-learning uses the maximum action value as an approximation for the maximum expected action value. The update of Q-learning is:</p>

<script type="math/tex; mode=display">Q_{t+1}(s_t,a_t)=Q_t(s_t,a_t)+\alpha_t(s_t,a_t)(r_t+\gamma \max_a Q_t(s_{t+1},a)-Q_t(s_t,a_t))</script>

<p>While this update method has a issue—overestimation, cause’ this formula always choose the maximum value of $Q_t(s_{t+1},a \mid a \in \mathcal{A}(s_{t+1}))$.</p>

<p><strong>Estimating the maximum expected value</strong></p>

<p>Assume that there has $M$ random variables</p>

<script type="math/tex; mode=display">X=\{ X_1, X_2,…,X_M \}</script>

<p>and the maximum expected value of thise valriable set is:</p>

<script type="math/tex; mode=display">\max_{i}E[X_i]</script>

<p>Cause we have no knowledge of the function form and parameters of the underlying distribution of the variables in $X$, so we need construct appoximations for $E[X_i]$ for all $i$. Assume we have a sample set $S=\bigcup^M_{i=1}S_i$, in which $S_i$ is a sample set of $X_i$. Also we assume that each $S_i$ obey the iid condition (独立同分布), and unbias estimation can be the average of each $S_i$, then we have:</p>

<script type="math/tex; mode=display">E[X_i]=E[\mu_i]\thickapprox\mu_i(S) \overset{def}{=}{ {1\over \vert S_i \vert}{\sum_{s\in S_i}s} }</script>

<p>Which $\mu_i$ represents the estimator of $X_i$, so, if we wanna get the $\max_i{E[X_i] }$, we can calculate the $\max \mu_i(S)$ to get the approximation, while $E{ \max_i \mu_i }$ is the unbias of $\max_i \mu_i$.</p>

<p>We introduce two conception, PDF (probability density function) $f_i(x)$ and CDF (cumulative distribution function) $F_i(x)$:</p>

<script type="math/tex; mode=display">F_i(x)=\int_{-\infty}^{x}f_i(x)dx</script>

<p>and $F_i(x) = \int_{-\infty}^{\infty}f_i(x)dx=1$, so, the CDF-formula of $\max_i{E[X_i] }$ is: $\max_i\int_{-\infty}^{\infty}f_i(x)dx$.</p>

<p><strong>Double Estimator</strong></p>

<p>Here, we assume there has two independent sample set $S_A$ and $S_B$, while $S_A=\bigcup_{i=1}^M{S_A^i}$ and $S_B=\bigcup_{i=1}^M{S_B^i}$, and $S_A^i \bigcap S_B^i=\emptyset$. As we mentioned above, we create estimatior $\mu_i^A$ or $\mu_i^B$ for each subset of $S_A$ or $S_B$.</p>

<p>There has some differences as we mentioned above, I believe you have detected them. Now, we will use one sample set to select the maximal estimates in one estimator set, suppose we select $S_A$ ans $\mu_A(S)$ to do that, then we define the maximal estimates:</p>

<script type="math/tex; mode=display">Max^A(S) \overset{def}{=} \{j \mid \mu_j^A(S)\}</script>

<p>If there has many $j$, then we select one from them randomly. And because $\mu^B$ is an independent, unbiased set of estimators, we have $E[\mu_j^B]=E[X_j]$ for all $j$, including all $j \in Max^A$. Let $a^{*}$ be the extimator that maximizes $\mu^A$: $\mu_{a^\star}^A(S)\overset{def}{=}\max_i\mu_{i}^A(S)$. Then we can use $\mu_{a^\star }^B$ as an estimator for $\max_iE[X_i]$, and we have:</p>

<script type="math/tex; mode=display">\max_iE[X_i]=max_iE[\mu_i^B]\thickapprox \mu_{a^\star}^B</script>

<p>Now, assume that the underlying PDFs are continuous, then the probability $P(j=a^{\star})$ equals to the probability that all $i \ne j$ give lower estimates. Thus $\mu_j^A(S)=x$ is maximal for some value $x$ with probability $\Pi_{i \ne j}^M{P(\mu_i^A&lt;x)}$, then $P(j=a^\star)$ is:</p>

<script type="math/tex; mode=display">% <![CDATA[
P(\mu_j^A=x)\Pi_{i\ne j}^M{P(\mu_i^A<x)} \overset{def}{=} \int_{-\infty}^{\infty}f_j^A(x)\Pi_{i\ne j}^M{F_i^A(x)dx} %]]></script>

<p>And $E{\mu_{a^{*} }^B}$ is a unbiased estimation of $\mu_{a^\star}^B$, the expectation of $\mu_{a^\star }^B$ is:</p>

<script type="math/tex; mode=display">\sum_{j}^M{P(j=a^{\star})E[\mu_j^B] }=\sum_{j}^M{E[\mu_j^B]\int_{-\infty}^{\infty}f_{j}^A(x)\Pi_{i\ne j}^M{F_i^A(x)dx} }</script>

<p><strong>Is unbias or lower than the unbias ?</strong></p>

<p>As we metioned above, we have</p>

<script type="math/tex; mode=display">E[X_i]=E[\mu_i^A]=E[\mu_i^B]</script>

<p>and let</p>

<script type="math/tex; mode=display">\mathcal{M}\overset{def}{=}\{j \mid E[X_j]=\max_iE[X_i]\}</script>

<p>be the set of elements that maximize the expectated values, so the expectated value can be written as：</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}E[\mu_{a^\star}^B]  &=P(a^{\star} \in \mathcal{M})E[\mu_{a^{\star} }^B \mid a^{\star} \in \mathcal{M}]+P(a^{\star} \notin \mathcal{M})E[\mu_{a^{\star} }^B \mid a^{\star} \notin \mathcal{M}] \\ &=P(a^{\star} \in \mathcal{M})\max_{i}E[X_i]+P(a^{\star} \notin \mathcal{M})E[\mu_{a^{\star} }^B \mid a^{\star} \notin \mathcal{M}] \\ &\le P(a^{\star} \in \mathcal{M})E[\mu_{a^{\star} }^B \mid a^{\star} \in \mathcal{M}]+P(a^{\star} \notin \mathcal{M})\max_i E[X_i] \\ &=\max_i{E[X_i] }\end{align} %]]></script>

<p>Where the inequality is strict if and only if $P(a^{\star} \notin \mathcal{M}) &gt; 0$. This happens when the variables have different expected values, but their distributions overlap. In contrast with the single estimator, the double estimator is unbiased when the variables are iid, since then all expected values are equal and $P(a^{\star} \in \mathcal{M})=1$.</p>

<p><strong>Prove convergence of Double Q-learning</strong></p>

<p>Although we solve the estimation issue, there has another big issue needs to be solved, that’s the convergence of Double Q-learning. Cause’ its convergenc e is inherited from Q-learning, so I will show the prove of convergence of Q-leraning (<em>also I acquiesce that you know the Double Q-learning algorithm and its update process</em>).</p>

<p>All we know that the update process of Q-learning is:</p>

<script type="math/tex; mode=display">Q(S_{t+1}, a_{t+1}) := Q(S_{t+1}, a_{t+1}) + \alpha(r+\gamma\max_{a}Q(S_{t}, a) - Q(S_{t+1}, a_{t+1}))</script>

<p>Now, let us define some ABC of Markov decision process $(\mathcal{X}, \mathcal{A}, \mathcal{P}, r)$, where</p>

<ul>
  <li>$\mathcal{X}$ is the finite state-space</li>
  <li>$\mathcal{A}$ is the finite action-space</li>
  <li>$\mathcal{P}$ represents the transition probabilities</li>
  <li>$r$ represents the reward function</li>
</ul>

<p>Then, we have the function $r$ defined as:</p>

<script type="math/tex; mode=display">r: \mathcal{X} \times \mathcal{A} \times \mathcal{X} \rightarrow \mathbb{R}</script>

<p>And the value of a state $x$ is defined for a sequence of controls ${A_t}$, as:</p>

<script type="math/tex; mode=display">J(x, \{A_t\})=\mathbb{E}\Bigg[\sum_{t=0}^{\infty}\gamma^tR(X_t,A_t) \mid X_0=x\Bigg]</script>

<p>The optimal value functions is defined, for each $x \in \mathcal{X}$ as:</p>

<script type="math/tex; mode=display">V^{\star}(x)=\max_{\mathcal{A} }J(x,\{A_t\})</script>

<p>and verifies</p>

<script type="math/tex; mode=display">V^{\star}(x)=\max_{a\in \mathcal{A}}\sum_{y\in \mathcal{X} }\mathbb{P}_a(x,y)[r(x,a,y)+\gamma V^{\star}(y)]</script>

<p>$x$ and $y$ represent two states which $x$ switch to $y$ through action $a$, and we call $(x, a, y)$ as a transition of Markov decision process $(\mathcal{X}, \mathcal{A}, \mathcal{P}, r)$.</p>

<p>And from above, we define the optimal Q-function, $Q^{\star}$ as:</p>

<script type="math/tex; mode=display">Q^{\star}(x, a)=\sum_{y\in \mathcal{X} }[r(x,a,y)+\gamma V^{\star}(y)]</script>

<p>We can define a contraction operator $\mathbb{H}$ which defined for a generic function: $q: \mathcal{X} \times \mathcal{A} \rightarrow \mathbb{R}$ as:</p>

<script type="math/tex; mode=display">(\mathbb{H}(x,a)=\sum_{y\in \mathcal{X} }\mathbb{P}_a(x,y)[r(x,a,y)+\gamma \max_{b\in \mathcal{A} }q(y,b)])</script>

<p>And the meaning of construction operator can be intrepreted as follow:</p>

<script type="math/tex; mode=display">\Vert \mathbb{H}q_1 - \mathbb{H}q_2 \Vert_{\infty} \le \Vert q_1 - q2\Vert_{\infty}</script>

<p>and we can prove it:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}\Vert \mathbb{H}q_1 - \mathbb{H}q_2 \Vert_{\infty} &= \max\Bigg\vert \sum_{y\in\mathcal{X} }[r(x,a,y)+\gamma \max_{b\in \mathcal{A} }q_1(y, b) - r(x,a,y) + \gamma\max_{b\in \mathcal{A} }q_2(y,b)]\Bigg\vert \\ &=\max_{x,a}\gamma \Bigg\vert \sum_{y\in \mathcal{X} }\mathbb{P}_a(x,y)[\max_{b\in \mathcal{A} }q_1(y,b)-\max_{b\in \mathcal{A} }q_2(y,b)] \Bigg\vert  \\  &\le\max_{x,a}\gamma \sum_{y\in \mathcal{X} }\mathbb{P}_a(x,y)\Bigg\vert \max_{b\in\mathcal{A} }q_1(y,b)-\max_{b\in\mathcal{A} }q_2(y,b) \Bigg\vert  \\  &\le \max_{x,a}\gamma \sum_{y\in \mathcal{X} }\mathbb{P}_a(x,y)\max_{z,b}\vert q_1(z,b)-q_2(z,b) \vert \\  &=\max_{x,a} \gamma \sum_{y\in \mathcal{X} }\mathbb{P}_a(x,y)\Vert q_1-q_2 \Vert_{\infty}  \\  &=\gamma\Vert q_1 - q_2\Vert_{\infty} \end{align} %]]></script>

<p>Back to the first formula at this block, as we see, if we wann $Q$-function converge, then we need promise that $\alpha(r+\gamma\max_{a}Q(S_{t}, a) - Q(S_{t+1}, a_{t+1}))$ should converge. So, it the key of convergence.</p>

<p>Let $F_t(x,a)=r(x,a,y)+\gamma\max_{b\in\mathcal{A} }Q_t(y, b) - Q^{\star}(x, a)$, so we have:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}\mathbb{E}[F_t(x,a) \mid \mathcal{F}_t] &=\sum_{y\in \mathcal{X} }\mathbb{P}_a(x,y)r(x,a,y)+\gamma\max_{b\in\mathcal{A} }Q_t(y, b) - Q^{\star}(x, a)] \\ &= (\mathbb{H}Q_t)(x,a)-Q^{\star}(x,a)\end{align} %]]></script>

<p>as we mentioned before, the operator $\mathbb{H}$ si a constraction operator, so we can rewrite the second item $Q^{\star}$ as $\mathbb{H}Q^{\star}$, and then we have:</p>

<script type="math/tex; mode=display">\mathbb{E}[F_t(x,a) \mid \mathcal{F}_t]=(\mathbb{H}Q_t)(x,a)-(\mathbb{H}Q^{\star})(x,a)</script>

<p>If we use the prove process above, we can get a conclusion easily:</p>

<script type="math/tex; mode=display">\Big\Vert\mathbb{E}[F_t(x,a) \mid \mathcal{F}_t]\Big\Vert=\Big\Vert(\mathbb{H}Q_t)(x,a)-(\mathbb{H}Q^{\star})(x,a)\Big\Vert_{\infty} \le \gamma\Vert Q_t-Q^{\star} \Vert_{\infty}</script>

<p>So far, we have proved the convergence of Q-learning, also Double Q-learning.</p>

<p><strong>References</strong></p>

<ol>
  <li><a href="https://papers.nips.cc/paper/3964-double-q-learning">Double Q-learning</a></li>
  <li><a href="http://users.isr.ist.utl.pt/~mtjspaan/readingGroup/ProofQlearning.pdf">Convergence of Q-learning</a></li>
</ol>

<p><em>if you have some questions, please contact me, especially the approach e-mail</em></p>

  </section>

  <div class="post-meta">
      <span class="post-meta__tags tags"><i class="fa fa-tags"></i> <a href="/tags/machine-learning/">Machine Learning</a>, <a href="/tags/reinforcement-learning/">Reinforcement Learning</a></span>
    </div>
</article>

<section class="read-more">
   
   
   <div class="read-more-item">
       <span class="read-more-item-dim">最近的文章</span>
       <h2 class="post-list__post-title post-title"><a href="/2017/12/Overview-Multi-Agent/" title="link to Overview - Multi-Agent">Overview - Multi-Agent</a></h2>
       <p class="excerpt">1. Multi-Agent reinforcement learning algorithms  keep tracking of the other agents’ policy for adaptation. (opponent modeling)  a fusion of temporal-different RL, game theory, and more general direct policy search techniques2. Fully cooperative t...&hellip;</p>
       <div class="post-list__meta"><time datetime="2017-12-14 15:35:00 +0800" class="post-list__meta--date date">2017-12-14</time> <a class="btn-border-small" href=/2017/12/Overview-Multi-Agent/>继续阅读</a></div>
   </div>
   
   
   
   
   <div class="read-more-item">
       <span class="read-more-item-dim">更早的文章</span>
       <h2 class="post-list__post-title post-title"><a href="/2017/11/Temporal-Difference-Learning/" title="link to Temporal-Difference Learning">Temporal-Difference Learning</a></h2>
       <p class="excerpt">TD learning是对Monte Carlo和DP算法的综合，像Monte Carlo一样属于一种Modle Free算法，即不需要对环境模型有一个具体的认识（比如需要知道状态转移的Markov决策过程）；同时也像DP一样属于一种迭代更新算法。也许这个表达式可以更容易地帮助理解：其中 $V(S_t)$ 表示在 $t$ 时刻状态为 $S$ 的 state value，在Monte Carlo里面，$V(S)$ 的估计通常采用 first visit 进行，以保证估计过程的 unbias ...&hellip;</p>
       <div class="post-list__meta"><time datetime="2017-11-03 02:00:00 +0800" class="post-list__meta--date date">2017-11-03</time><a class="btn-border-small" href=/2017/11/Temporal-Difference-Learning/>继续阅读</a></div>
   </div>
   
</section>

<section class="post-comments">
  
    <div id="disqus_thread"></div>
    <script>
    
    var disqus_config = function () {
        this.page.url = "http://kornbergfresnel.github.io/2017/11/Prove-of-Double-Q-learning/";
        this.page.identifier = "/2017/11/Prove-of-Double-Q-learning/";
    };

    var disqus_shortname = 'vno-jekyll';
    
    (function() { // DON'T EDIT BELOW THIS LINE
        var d = document, s = d.createElement('script');
        s.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();
    </script>
    <noscript>要查看<a href="http://disqus.com/?ref_noscript"> Disqus </a>评论，请启用 JavaScript</noscript>
    
  
  
  
  
</section>


            <section class="footer">
    <footer>
    	<span class="footer__copyright">本站点采用<a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">知识共享 署名-非商业性使用-相同方式共享 4.0 国际 许可协议</a></span>
        <span class="footer__copyright">由 <a href="https://jekyllrb.com">Jekyll</a> 于 2018-01-06 生成，感谢 <a href="https://www.digitalocean.com/?refcode=30ed2d146762">Digital Ocean</a> 为本站提供稳定的 VPS 服务</span>
        <span class="footer__copyright">本站由 <a href="http://twitter/andrew_kornberg">@andrew_kornberg</a> 创建，采用 <a href="https://github.com/onevcat/vno-jekyll">Vno - Jekyll</a> 作为主题，您可以在 GitHub 找到<a href="https://github.com/onevcat/OneV-s-Den">本站源码</a> - &copy; 2018</span>
    </footer>
</section>

        </div>
    </div>
    
    <script type="text/javascript" src="//code.jquery.com/jquery-1.11.3.min.js"></script>

<script type="text/javascript" src="/js/main.js"></script>



    
  </body>

</html>
