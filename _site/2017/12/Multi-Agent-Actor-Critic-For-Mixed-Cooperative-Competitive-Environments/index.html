<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <title>Multi-Agent Actor-Critic For Mixed Cooperative-Competitive Environments</title>
  <meta name="description" content="Why propose this framework for Multi-Agent ?">
  <meta name="author" content="Wei Wang">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Multi-Agent Actor-Critic For Mixed Cooperative-Competitive Environments">
  <meta name="twitter:description" content="Why propose this framework for Multi-Agent ?">
  
  <meta property="og:type" content="article">
  <meta property="og:title" content="Multi-Agent Actor-Critic For Mixed Cooperative-Competitive Environments">
  <meta property="og:description" content="Why propose this framework for Multi-Agent ?">
  
  <link rel="icon" type="image/png" href="/assets/images/favicon.png" />
  <link href="/assets/images/favicon.png" rel="shortcut icon" type="image/png">
  
  <link rel="stylesheet" href="/css/main.css">
  <link href="//netdna.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">

  <link rel="canonical" href="http://kornbergfresnel.github.io/2017/12/Multi-Agent-Actor-Critic-For-Mixed-Cooperative-Competitive-Environments/">
  <link rel="alternate" type="application/rss+xml" title="Andrew Kornberg" href="http://kornbergfresnel.github.io/feed.xml">
  
  <meta name="google-site-verification" content="1-1ZlHoRvM0T2FqPbW2S-qLgYXN6rsn52kErlMPd_gw" />

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>  
  
</head>


  <body>

    <span class="mobile btn-mobile-menu">
        <i class="fa fa-list btn-mobile-menu__icon"></i>
        <i class="fa fa-angle-up btn-mobile-close__icon hidden"></i>
    </span>
    
    <header class="panel-cover panel-cover--collapsed" style="background-image: url('/assets/images/background-cover.jpg')">
  <div class="panel-main">

    <div class="panel-main__inner panel-inverted">
    <div class="panel-main__content">

        <a href="/#blog" title="前往 Andrew Kornberg 的主页" class="blog-button"><img src="/assets/images/avatar.jpg" width="80" alt="Andrew Kornberg logo" class="panel-cover__logo logo" /></a>
        <h1 class="panel-cover__title panel-title"><a href="/#blog" title="link to homepage for Andrew Kornberg" class="blog-button">Andrew Kornberg</a></h1>
        
        <span class="panel-cover__subtitle panel-subtitle">abeunt studia in morse</span>
        
        <hr class="panel-cover__divider" />
        <p class="panel-cover__description">Hi, I'm Kornberg, a wild programmer & PhD. candidate</p>
        <hr class="panel-cover__divider panel-cover__divider--secondary" />
        
        
        
        <div class="navigation-wrapper">
          <div>
            <nav class="cover-navigation cover-navigation--primary">
              <ul class="navigation">
                <li class="navigation__item"><a href="/#blog" title="Visit blog" class="blog-button">Blog</a></li>
                
                  <li class="navigation__item"><a href="http://kornbergfresnel.github.io" target="_blank" title="My open-source projects">Projects</a></li>
                
              </ul>
            </nav>
          </div>
          
          <div><nav class="cover-navigation navigation--social">
  <ul class="navigation">

  
  <!-- Weibo -->
  <li class="navigation__item">
    <a href="http://weibo.com/u/5104230793" title="@u/5104230793 的微博" target="_blank">
      <i class='social fa fa-weibo'></i>
      <span class="label">Weibo</span>
    </a>
  </li>
  

  
  <!-- Github -->
  <li class="navigation__item">
    <a href="https://github.com/KornbergFresnel" title="@KornbergFresnel 的 Github" target="_blank">
      <i class='social fa fa-github'></i>
      <span class="label">Github</span>
    </a>
  </li>
  
  
  
  <!-- Twitter -->
  <li class="navigation__item">
    <a href="http://twitter.com/andrew_kornberg" title="@andrew_kornberg" target="_blank">
      <i class='social fa fa-twitter'></i>
      <span class="label">Twitter</span>
    </a>
  </li>
  

  
  <!-- Google Plus -->
  <li class="navigation__item">
    <a href="https://plus.google.com/107108267983477358170" rel="author" title="Google+" target="_blank">
      <i class='social fa fa-google-plus-square'></i>
      <span class="label">Google Plus</span>
    </a>
  </li>
  

  <!-- RSS -->
  <li class="navigation__item">
    <a href="/feed.xml" rel="author" title="RSS" target="_blank">
      <i class='social fa fa-rss'></i>
      <span class="label">RSS</span>
    </a>
  </li>

  
  <!-- Email -->
  <li class="navigation__item">
    <a href="mailto:kornbergfresnel@outlook.com" title="Contact me">
      <i class='social fa fa-envelope'></i>
      <span class="label">Email</span>
    </a>
  </li>
  

  </ul>
</nav>
</div>
        </div>
      </div>
    </div>
    
    
    <div class="panel-cover--overlay cover-blue"></div>
    
  </div>
</header>


    <div class="content-wrapper">
        <div class="content-wrapper__inner">
            

<article class="post-container post-container--single" itemscope itemtype="http://schema.org/BlogPosting">
  <header class="post-header">
    <div class="post-meta">
      <time datetime="2017-12-19 00:00:00 +0800" itemprop="datePublished" class="post-meta__date date">2017-12-19</time> &#8226; <span class="post-meta__tags tags"><i class="fa fa-tags"></i> <a href="/tags/paper/">Paper</a>, <a href="/tags/multi-agent/">Multi-Agent</a>, <a href="/tags/reinforcement-learning/">Reinforcement Learning</a></span>
    </div>
    <h1 class="post-title">Multi-Agent Actor-Critic For Mixed Cooperative-Competitive Environments</h1>
  </header>

  <section class="post">
    <p><strong>Why propose this framework for Multi-Agent ?</strong></p>

<ul>
  <li>Q-learning is not effective under non-stationary environment</li>
  <li>policy-gradient suffers from a variance that increase as the number of agents grows</li>
  <li>at this paper, authors proposal an adaptation of actor-critic methods that consider <em>action policies of other agents</em></li>
</ul>

<p><strong>Why the former methods are poorly suited to multi-agent environments ?</strong></p>

<ul>
  <li>non-stationary cause the changes in the agents’ own policies(that is not explaineable), and this issue also become a challenge of learning stability and prevents the straightforward use of past experience replay, which is crucial for stabilizing deep Q-learning</li>
  <li>in multi-agent system, it is common that coordination is required while policy gradient suffers from high variance</li>
</ul>

<p><strong>How do we do ?</strong></p>

<blockquote>
  <p>the authors in this paper propose a general-purpose multi-agent learning algorithm</p>
</blockquote>

<ol>
  <li>learned policies only use local information at execution time</li>
  <li>no explicitly communication structure: does not assume a differentiable model of the environment dynamics or any particular structure on the communication method between agents</li>
  <li>not only cooperative, but also competitive or mixed task, for this setting is more natural</li>
</ol>

<p><strong>Differ with centralized critic function</strong></p>

<ul>
  <li>
    <p>critic use some addtional information while do not use at test time, in this paper, authors use the policies of other agents</p>
  </li>
  <li>the centralized critic function explicitly use the decision-making policies of the other agents, while the authors of this paper let agents learn <em>approximate models of other agents online and effictively use them in their own policy learning procedure</em></li>
  <li>introduce a method to improve the stability of multi-agent policy by training agent with an <strong>ensemble of policies</strong> (<strong>QUESTION</strong>: use the ensemble policies to do what ? RL ?)</li>
</ul>

<p><strong>How ensemble policies work ?</strong></p>

<ul>
  <li>still the issue — <strong>non-stationary</strong>. For the non-stationary, the agents’ policies always change, and under the setting of competitive, it is true that agents can derive a strong policy by overfitting to the behavior of their competitors (<strong>WHY?</strong>)</li>
  <li>for learning a more robust model, training a collection of $K$ different sub-policies will work</li>
  <li>training collecton of $K$ different sub-policies
    <ul>
      <li>at each episode, randomly select one particular sub-policy $\mu_i^{(k)}$ for each agent to execute</li>
      <li>for agent $i$, maximizing the ensemble objective function: $J_e(\mu_i)=\mathbb{E}_{k \sim unif(1,K),s \sim p^\mu, a \sim \mu_i^{(k)} }[R_i(s,a)] $</li>
      <li><strong>QUESTION</strong>: how to matain the collection of sub-policy</li>
    </ul>
  </li>
</ul>

<p><strong>DPG(Deterministic Policy Gradient) Algorithms</strong></p>

<p>it is alsow posible to extend the policy gradient framework to deterministic policies $\mu_{\theta}: \mathcal{S} \rightarrow \mathcal{A}$. So we can rewrite the objective function $J$ as:</p>

<script type="math/tex; mode=display">\Delta_{\theta}J(\theta)=\mathbb{E}_{s\sim \mathcal{D} }[\nabla_{\theta}\mu_{\theta}(a \mid s)\nabla_aQ^{\mu}(s,a) \mid_{a=\mu_{\theta}(s) }]</script>

<p>While the derive $\nabla_aQ$ implies that the action space should be continuous. <em>DDPG(Deep deterministic policy gradient)</em> is a variant of DPG where the policy $\mu$ and critic $Q^{\mu}$ are approximated with deep neural networks. DDPG is an off-policy algorithm, and samples trajectories from a replay buffer of experiences that are stored throughout training. DDPG also makes use of a target network, as in DQN</p>

<p><strong>Multi-Agent Actor Critic</strong></p>

<blockquote>
  <p>follow the above setting, the authors of this paper propose a simple extension of actor-critic policy gradient methods where the critic is augmented with the extra information about the policies of other agents</p>
</blockquote>

<ul>
  <li>suppose a game with $N$ agents with policies $\pi={\pi_1,\pi_2,…,\pi_n}$ parameterized with $\theta={\theta_1,\theta_2,…,\theta_n}$</li>
  <li>the actor learn a $Q$ function for each agent $i$ which accepts actions of all agents with some state information</li>
</ul>

<p><strong>MADDPG</strong></p>

<ul>
  <li>a primary motivation behind MADDPG is that if we know the actions taken by all agents, the environment is stationary event as the policies change.</li>
  <li>while under the real conditions, we cannot know the actions of other agents, also their observation or policies, but we can approximate their policies from their observations.</li>
  <li>then we can replace the real action input of critic with approximation action: $\hat{y}=r_i+\gamma Q_i^{\mu’}{(x’, \mu_i^{‘1}(o1)),\mu_i^{‘2}(o2),…,\mu_i^{‘N}(oN))}$</li>
  <li>​</li>
</ul>

  </section>

  <div class="post-meta">
      <span class="post-meta__tags tags"><i class="fa fa-tags"></i> <a href="/tags/paper/">Paper</a>, <a href="/tags/multi-agent/">Multi-Agent</a>, <a href="/tags/reinforcement-learning/">Reinforcement Learning</a></span>
    </div>
</article>

<section class="read-more">
   
   
   
   
   <div class="read-more-item">
       <span class="read-more-item-dim">更早的文章</span>
       <h2 class="post-list__post-title post-title"><a href="/2017/12/Analysis-Fictitious-Self-Play-in-Extensive-Form/" title="link to Analysis - Fictitious Self-Play in Extensive-Form Games">Analysis - Fictitious Self-Play in Extensive-Form Games</a></h2>
       <p class="excerpt">这是Deep Mind于2015年发表在JMLR上的一篇文章，文章提出了fictitious play的两种变体方法，使得fictious play能够在大型问题中得到有效的应用。这两种方法在文中分别被称为Full-width extensive-form play（XFP）和Fictitious self-play（FSP）。其实现都是基于行为策略，使用这种策略的一个好处是，可以极大的减少需要参考的动作空间和状态数量，从而减轻模拟过程或者算法对计算资源的消耗，学习速率当然也会有提升。De...&hellip;</p>
       <div class="post-list__meta"><time datetime="2017-12-17 00:00:00 +0800" class="post-list__meta--date date">2017-12-17</time><a class="btn-border-small" href=/2017/12/Analysis-Fictitious-Self-Play-in-Extensive-Form/>继续阅读</a></div>
   </div>
   
</section>

<section class="post-comments">
  
    <div id="disqus_thread"></div>
    <script>
    
    var disqus_config = function () {
        this.page.url = "http://kornbergfresnel.github.io/2017/12/Multi-Agent-Actor-Critic-For-Mixed-Cooperative-Competitive-Environments/";
        this.page.identifier = "/2017/12/Multi-Agent-Actor-Critic-For-Mixed-Cooperative-Competitive-Environments/";
    };

    var disqus_shortname = 'vno-jekyll';
    
    (function() { // DON'T EDIT BELOW THIS LINE
        var d = document, s = d.createElement('script');
        s.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();
    </script>
    <noscript>要查看<a href="http://disqus.com/?ref_noscript"> Disqus </a>评论，请启用 JavaScript</noscript>
    
  
  
  
  
</section>


            <section class="footer">
    <footer>
    	<span class="footer__copyright">本站点采用<a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">知识共享 署名-非商业性使用-相同方式共享 4.0 国际 许可协议</a></span>
        <span class="footer__copyright">由 <a href="https://jekyllrb.com">Jekyll</a> 于 2018-01-06 生成，感谢 <a href="https://www.digitalocean.com/?refcode=30ed2d146762">Digital Ocean</a> 为本站提供稳定的 VPS 服务</span>
        <span class="footer__copyright">本站由 <a href="http://twitter/andrew_kornberg">@andrew_kornberg</a> 创建，采用 <a href="https://github.com/onevcat/vno-jekyll">Vno - Jekyll</a> 作为主题，您可以在 GitHub 找到<a href="https://github.com/onevcat/OneV-s-Den">本站源码</a> - &copy; 2018</span>
    </footer>
</section>

        </div>
    </div>
    
    <script type="text/javascript" src="//code.jquery.com/jquery-1.11.3.min.js"></script>

<script type="text/javascript" src="/js/main.js"></script>



    
  </body>

</html>
