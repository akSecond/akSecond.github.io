<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <title>Overview - Multi-Agent</title>
  <meta name="description" content="1. Multi-Agent reinforcement learning algorithms">
  <meta name="author" content="Wei Wang">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Overview - Multi-Agent">
  <meta name="twitter:description" content="1. Multi-Agent reinforcement learning algorithms">
  
  <meta property="og:type" content="article">
  <meta property="og:title" content="Overview - Multi-Agent">
  <meta property="og:description" content="1. Multi-Agent reinforcement learning algorithms">
  
  <link rel="icon" type="image/png" href="/assets/images/favicon.png" />
  <link href="/assets/images/favicon.png" rel="shortcut icon" type="image/png">
  
  <link rel="stylesheet" href="/css/main.css">
  <link href="//netdna.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">

  <link rel="canonical" href="http://kornbergfresnel.github.io/2017/12/Overview-Multi-Agent/">
  <link rel="alternate" type="application/rss+xml" title="Andrew Kornberg" href="http://kornbergfresnel.github.io/feed.xml">
  
  <meta name="google-site-verification" content="1-1ZlHoRvM0T2FqPbW2S-qLgYXN6rsn52kErlMPd_gw" />

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>  
  
</head>


  <body>

    <span class="mobile btn-mobile-menu">
        <i class="fa fa-list btn-mobile-menu__icon"></i>
        <i class="fa fa-angle-up btn-mobile-close__icon hidden"></i>
    </span>
    
    <header class="panel-cover panel-cover--collapsed" style="background-image: url('/assets/images/background-cover.jpg')">
  <div class="panel-main">

    <div class="panel-main__inner panel-inverted">
    <div class="panel-main__content">

        <a href="/#blog" title="前往 Andrew Kornberg 的主页" class="blog-button"><img src="/assets/images/avatar.jpg" width="80" alt="Andrew Kornberg logo" class="panel-cover__logo logo" /></a>
        <h1 class="panel-cover__title panel-title"><a href="/#blog" title="link to homepage for Andrew Kornberg" class="blog-button">Andrew Kornberg</a></h1>
        
        <span class="panel-cover__subtitle panel-subtitle">abeunt studia in morse</span>
        
        <hr class="panel-cover__divider" />
        <p class="panel-cover__description">Hi, I'm Kornberg, a wild programmer & PhD. candidate</p>
        <hr class="panel-cover__divider panel-cover__divider--secondary" />
        
        
        
        <div class="navigation-wrapper">
          <div>
            <nav class="cover-navigation cover-navigation--primary">
              <ul class="navigation">
                <li class="navigation__item"><a href="/#blog" title="Visit blog" class="blog-button">Blog</a></li>
                
                  <li class="navigation__item"><a href="http://kornbergfresnel.github.io" target="_blank" title="My open-source projects">Projects</a></li>
                
              </ul>
            </nav>
          </div>
          
          <div><nav class="cover-navigation navigation--social">
  <ul class="navigation">

  
  <!-- Weibo -->
  <li class="navigation__item">
    <a href="http://weibo.com/u/5104230793" title="@u/5104230793 的微博" target="_blank">
      <i class='social fa fa-weibo'></i>
      <span class="label">Weibo</span>
    </a>
  </li>
  

  
  <!-- Github -->
  <li class="navigation__item">
    <a href="https://github.com/KornbergFresnel" title="@KornbergFresnel 的 Github" target="_blank">
      <i class='social fa fa-github'></i>
      <span class="label">Github</span>
    </a>
  </li>
  
  
  
  <!-- Twitter -->
  <li class="navigation__item">
    <a href="http://twitter.com/andrew_kornberg" title="@andrew_kornberg" target="_blank">
      <i class='social fa fa-twitter'></i>
      <span class="label">Twitter</span>
    </a>
  </li>
  

  
  <!-- Google Plus -->
  <li class="navigation__item">
    <a href="https://plus.google.com/107108267983477358170" rel="author" title="Google+" target="_blank">
      <i class='social fa fa-google-plus-square'></i>
      <span class="label">Google Plus</span>
    </a>
  </li>
  

  <!-- RSS -->
  <li class="navigation__item">
    <a href="/feed.xml" rel="author" title="RSS" target="_blank">
      <i class='social fa fa-rss'></i>
      <span class="label">RSS</span>
    </a>
  </li>

  
  <!-- Email -->
  <li class="navigation__item">
    <a href="mailto:kornbergfresnel@outlook.com" title="Contact me">
      <i class='social fa fa-envelope'></i>
      <span class="label">Email</span>
    </a>
  </li>
  

  </ul>
</nav>
</div>
        </div>
      </div>
    </div>
    
    
    <div class="panel-cover--overlay cover-blue"></div>
    
  </div>
</header>


    <div class="content-wrapper">
        <div class="content-wrapper__inner">
            

<article class="post-container post-container--single" itemscope itemtype="http://schema.org/BlogPosting">
  <header class="post-header">
    <div class="post-meta">
      <time datetime="2017-12-14 15:35:00 +0800" itemprop="datePublished" class="post-meta__date date">2017-12-14</time> &#8226; <span class="post-meta__tags tags"><i class="fa fa-tag"></i> <a href="/tags/multi-agent/">Multi-Agent</a></span>
    </div>
    <h1 class="post-title">Overview - Multi-Agent</h1>
  </header>

  <section class="post">
    <p><strong>1. Multi-Agent reinforcement learning algorithms</strong></p>

<ul>
  <li>keep tracking of the other agents’ policy for adaptation. (opponent modeling)</li>
  <li>a fusion of temporal-different RL, game theory, and more general direct policy search techniques</li>
</ul>

<p><strong>2. Fully cooperative tasks</strong></p>

<ul>
  <li>in the absence of additional coordination mechanisms, different agents may break these ties among multiple optimal joint actions in different ways,  and the resulting joint action may be suboptimal</li>
  <li>the Team Q-learning algorithm avoids the coordination problem by assuming that the optimal joint actions are unique (which is not always the case)</li>
  <li>Distributed Q-learning algorithm and Frequency Maximum Q-value both work under deterministic tasks or static tasks</li>
</ul>

<p><strong>3. A general way to solving the coordination problem is to make sure that any ties are broken by all the agents in the same way</strong></p>

<ul>
  <li>Social conventions encode a priori preferences toward certain joint actions, and help break ties during action selection.</li>
  <li>communication can be used to negotiate action choices, partial or complete q-tables, state-measurements, rewards, learning parameters, etc.</li>
</ul>

<p><strong>what’s the meaning of breaking tie ?</strong></p>

<p><em>break ties among multiple optimal joint actions in different ways, the tie means joint actions array</em></p>

<p><strong>4. Mixed tasks: no constraints are imposed on the reward functions of the agents, this model is most for self-interested (自利) (but not necessarily competing) agents</strong></p>

<ul>
  <li>
    <p>there still has some difficulties for the dynamic behavior of the agents (nonstationary). This is why most of the methods in this category focus on adaptation to the other agents</p>
  </li>
  <li>
    <p><strong>single-agent RL algorithms</strong> like Q-learning can be directly applied to the multi-agent case for that these algorithms learning without being aware of (意识到) the other agents, so the nonstationary of the MARL problem invalidates most of the single-agent RL theoretical guarantees, and also for their simplicity. But it appears that for certain parameter settings, Q-learning is able to converge to a coordinated equilibrium in particular games, in other cases, unfortunately, Q-learners exhibit non-stationary cyclic behavior.</p>
  </li>
  <li>
    <p><strong>the agent-independent methods</strong> has the similar structure like fully-competitive task, but there has a difference in the solver</p>
  </li>
  <li>in current method, solver(i) returns agent i’s part of some type of equilibrium (a strategy), and eval(i) gives the agent’s expected return given this equilibrium, <em>the goal is to converge to an equilibrium in every state.</em>
    <ul>
      <li>the update requires all agents use the same algorithm for measure all actions and rewards, but it only guaranteed to maintain identical results for all the agents when solve returns consistent equilibrium strategies for all the agents</li>
      <li><strong>so there will have a selection problem arises when the solution of solve is not unique</strong></li>
    </ul>
  </li>
  <li>
    <p>in the agent-independent methods, there has some external mechanism (coordination or negotiation) to guarantee the convergence of NE selection, such as <strong>correlated equilibrium Q-learning &amp; asymmetric Q-learning</strong></p>
  </li>
  <li><strong>agent-tracking methods</strong> estimate the policies of models (consider static or dynamic), and this category requires a <em>best-response</em> rather convergence to stationary strategies, and each agent is assumed capable to observe the other agents’ actions
    <ul>
      <li>under the static tasks, there has some important algorithms: MetaStrategy, Hyper-Q</li>
      <li>under the dynamic tasks, there has some important algorithms: Non-Stationary Converging Policies</li>
    </ul>
  </li>
  <li><strong>agent-aware (感知) methods target convergence, as well as adaptation to the other agents</strong>
    <ul>
      <li>under the static tasks, there has some important algorithms: AWESOME</li>
      <li>under the dynamic tasks, there has some important algorithms: Win-or-Learn-Fast Policy Hill-Climbing (WoLF-PHC)</li>
    </ul>
  </li>
</ul>

<p><strong>5. Application domains: distributed control, multi-robot teams, trading agents, and resource management</strong></p>

<ul>
  <li>distributed control is a meta-application for cooperative multi-agent systems, agents are controller, and their environment is the controlled process</li>
  <li>providing domain knowledge to the agents can greatly help them to learn solutions of realistic tasks, and domain knowledge can be supplied in several forms: informative reward functions, rewarding promising behaviors rather than just the achievement of the goal</li>
  <li>so far, game-theory-based analysis has only been applied to the learning dynamics of the agents, while the dynamics of the environment have not been explicitly considered.</li>
</ul>

<p><strong>6. Related work &amp; Extensive overview of MARL</strong></p>

<ul>
  <li>rather than estimating value functions and using them to derive policies, it is also possible to directly explore the space of agent behaviors using, e.g., nonlinear optimization techniques.</li>
  <li>MARL aims to provide an array of algorithms that enable multiple agents to learn the solution of difficult tasks, using limited or no prior knowledge.</li>
</ul>

  </section>

  <div class="post-meta">
      <span class="post-meta__tags tags"><i class="fa fa-tag"></i> <a href="/tags/multi-agent/">Multi-Agent</a></span>
    </div>
</article>

<section class="read-more">
   
   
   <div class="read-more-item">
       <span class="read-more-item-dim">最近的文章</span>
       <h2 class="post-list__post-title post-title"><a href="/2017/12/Analysis-Fictitious-Self-Play-in-Extensive-Form/" title="link to Analysis - Fictitious Self-Play in Extensive-Form Games">Analysis - Fictitious Self-Play in Extensive-Form Games</a></h2>
       <p class="excerpt">这是Deep Mind于2015年发表在JMLR上的一篇文章，文章提出了fictitious play的两种变体方法，使得fictious play能够在大型问题中得到有效的应用。这两种方法在文中分别被称为Full-width extensive-form play（XFP）和Fictitious self-play（FSP）。其实现都是基于行为策略，使用这种策略的一个好处是，可以极大的减少需要参考的动作空间和状态数量，从而减轻模拟过程或者算法对计算资源的消耗，学习速率当然也会有提升。De...&hellip;</p>
       <div class="post-list__meta"><time datetime="2017-12-17 00:00:00 +0800" class="post-list__meta--date date">2017-12-17</time> <a class="btn-border-small" href=/2017/12/Analysis-Fictitious-Self-Play-in-Extensive-Form/>继续阅读</a></div>
   </div>
   
   
   
   
   <div class="read-more-item">
       <span class="read-more-item-dim">更早的文章</span>
       <h2 class="post-list__post-title post-title"><a href="/2017/11/Prove-of-Double-Q-learning/" title="link to Prove of Double Q-learning">Prove of Double Q-learning</a></h2>
       <p class="excerpt">In some stochastic environments the well-known reinforcement learning algorithm Q-learning performs very poorly. This poor performance is caused by large overestimations of action values. These overestimations result from a positive bias that is i...&hellip;</p>
       <div class="post-list__meta"><time datetime="2017-11-14 00:20:00 +0800" class="post-list__meta--date date">2017-11-14</time><a class="btn-border-small" href=/2017/11/Prove-of-Double-Q-learning/>继续阅读</a></div>
   </div>
   
</section>

<section class="post-comments">
  
    <div id="disqus_thread"></div>
    <script>
    
    var disqus_config = function () {
        this.page.url = "http://kornbergfresnel.github.io/2017/12/Overview-Multi-Agent/";
        this.page.identifier = "/2017/12/Overview-Multi-Agent/";
    };

    var disqus_shortname = 'vno-jekyll';
    
    (function() { // DON'T EDIT BELOW THIS LINE
        var d = document, s = d.createElement('script');
        s.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();
    </script>
    <noscript>要查看<a href="http://disqus.com/?ref_noscript"> Disqus </a>评论，请启用 JavaScript</noscript>
    
  
  
  
  
</section>


            <section class="footer">
    <footer>
    	<span class="footer__copyright">本站点采用<a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">知识共享 署名-非商业性使用-相同方式共享 4.0 国际 许可协议</a></span>
        <span class="footer__copyright">由 <a href="https://jekyllrb.com">Jekyll</a> 于 2018-01-06 生成，感谢 <a href="https://www.digitalocean.com/?refcode=30ed2d146762">Digital Ocean</a> 为本站提供稳定的 VPS 服务</span>
        <span class="footer__copyright">本站由 <a href="http://twitter/andrew_kornberg">@andrew_kornberg</a> 创建，采用 <a href="https://github.com/onevcat/vno-jekyll">Vno - Jekyll</a> 作为主题，您可以在 GitHub 找到<a href="https://github.com/onevcat/OneV-s-Den">本站源码</a> - &copy; 2018</span>
    </footer>
</section>

        </div>
    </div>
    
    <script type="text/javascript" src="//code.jquery.com/jquery-1.11.3.min.js"></script>

<script type="text/javascript" src="/js/main.js"></script>



    
  </body>

</html>
