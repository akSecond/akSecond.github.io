<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <title>LSTM Tutorial</title>
  <meta name="description" content="  the LSTM network includes a set of recurrently connected subnets, which we call as “memory block”, these memory block are made with one or more self-connct...">
  <meta name="author" content="Wei Wang">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="LSTM Tutorial">
  <meta name="twitter:description" content="  the LSTM network includes a set of recurrently connected subnets, which we call as “memory block”, these memory block are made with one or more self-connct...">
  
  <meta property="og:type" content="article">
  <meta property="og:title" content="LSTM Tutorial">
  <meta property="og:description" content="  the LSTM network includes a set of recurrently connected subnets, which we call as “memory block”, these memory block are made with one or more self-connct...">
  
  <link rel="icon" type="image/png" href="/assets/images/favicon.png" />
  <link href="/assets/images/favicon.png" rel="shortcut icon" type="image/png">
  
  <link rel="stylesheet" href="/css/main.css">
  <link href="//netdna.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">

  <link rel="canonical" href="http://kornbergfresnel.github.io/2017/10/LSTM-Tutorial/">
  <link rel="alternate" type="application/rss+xml" title="Andrew Kornberg" href="http://kornbergfresnel.github.io/feed.xml">
  
  <meta name="google-site-verification" content="1-1ZlHoRvM0T2FqPbW2S-qLgYXN6rsn52kErlMPd_gw" />

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>  
  
</head>


  <body>

    <span class="mobile btn-mobile-menu">
        <i class="fa fa-list btn-mobile-menu__icon"></i>
        <i class="fa fa-angle-up btn-mobile-close__icon hidden"></i>
    </span>
    
    <header class="panel-cover panel-cover--collapsed" style="background-image: url('/assets/images/background-cover.jpg')">
  <div class="panel-main">

    <div class="panel-main__inner panel-inverted">
    <div class="panel-main__content">

        <a href="/#blog" title="前往 Andrew Kornberg 的主页" class="blog-button"><img src="/assets/images/avatar.jpg" width="80" alt="Andrew Kornberg logo" class="panel-cover__logo logo" /></a>
        <h1 class="panel-cover__title panel-title"><a href="/#blog" title="link to homepage for Andrew Kornberg" class="blog-button">Andrew Kornberg</a></h1>
        
        <span class="panel-cover__subtitle panel-subtitle">abeunt studia in morse</span>
        
        <hr class="panel-cover__divider" />
        <p class="panel-cover__description">Hi, I'm Kornberg, a wild programmer & PhD. candidate</p>
        <hr class="panel-cover__divider panel-cover__divider--secondary" />
        
        
        
        <div class="navigation-wrapper">
          <div>
            <nav class="cover-navigation cover-navigation--primary">
              <ul class="navigation">
                <li class="navigation__item"><a href="/#blog" title="Visit blog" class="blog-button">Blog</a></li>
                
                  <li class="navigation__item"><a href="http://kornbergfresnel.github.io" target="_blank" title="My open-source projects">Projects</a></li>
                
              </ul>
            </nav>
          </div>
          
          <div><nav class="cover-navigation navigation--social">
  <ul class="navigation">

  
  <!-- Weibo -->
  <li class="navigation__item">
    <a href="http://weibo.com/u/5104230793" title="@u/5104230793 的微博" target="_blank">
      <i class='social fa fa-weibo'></i>
      <span class="label">Weibo</span>
    </a>
  </li>
  

  
  <!-- Github -->
  <li class="navigation__item">
    <a href="https://github.com/KornbergFresnel" title="@KornbergFresnel 的 Github" target="_blank">
      <i class='social fa fa-github'></i>
      <span class="label">Github</span>
    </a>
  </li>
  
  
  
  <!-- Twitter -->
  <li class="navigation__item">
    <a href="http://twitter.com/andrew_kornberg" title="@andrew_kornberg" target="_blank">
      <i class='social fa fa-twitter'></i>
      <span class="label">Twitter</span>
    </a>
  </li>
  

  
  <!-- Google Plus -->
  <li class="navigation__item">
    <a href="https://plus.google.com/107108267983477358170" rel="author" title="Google+" target="_blank">
      <i class='social fa fa-google-plus-square'></i>
      <span class="label">Google Plus</span>
    </a>
  </li>
  

  <!-- RSS -->
  <li class="navigation__item">
    <a href="/feed.xml" rel="author" title="RSS" target="_blank">
      <i class='social fa fa-rss'></i>
      <span class="label">RSS</span>
    </a>
  </li>

  
  <!-- Email -->
  <li class="navigation__item">
    <a href="mailto:kornbergfresnel@outlook.com" title="Contact me">
      <i class='social fa fa-envelope'></i>
      <span class="label">Email</span>
    </a>
  </li>
  

  </ul>
</nav>
</div>
        </div>
      </div>
    </div>
    
    
    <div class="panel-cover--overlay cover-blue"></div>
    
  </div>
</header>


    <div class="content-wrapper">
        <div class="content-wrapper__inner">
            

<article class="post-container post-container--single" itemscope itemtype="http://schema.org/BlogPosting">
  <header class="post-header">
    <div class="post-meta">
      <time datetime="2017-10-02 16:41:00 +0800" itemprop="datePublished" class="post-meta__date date">2017-10-02</time> &#8226; <span class="post-meta__tags tags"><i class="fa fa-tags"></i> <a href="/tags/machine-learning/">Machine Learning</a>, <a href="/tags/machine-learning/">Machine Learning</a></span>
    </div>
    <h1 class="post-title">LSTM Tutorial</h1>
  </header>

  <section class="post">
    <blockquote>
  <p>the LSTM network includes a set of <strong>recurrently connected subnets</strong>, which we call as “<em>memory block</em>”, these memory block are made with one or more self-conncted memory cells and three multiplicative units—the input, output, and forget gates—that provide continuous analogues of write, read and reset operations for the cells.</p>
</blockquote>

<p><strong><em>what is the sensitivity in NN cell?</em></strong></p>

<p>The sensitivity of our NN cell has a connection with the impact of information (or input data) to current NN cell, if the sensitivity of a NN cell is low, we gonna say that current NN cell do not care about current input data.</p>

<p><strong><em>why RNNs have the vanishing gradient problem?</em></strong></p>

<p>We have some ways to make a statement for this problem.  In the aspect of transfering, the sensitivity of our network will decay over time as new inputs overwrite the activations of the hidden layer and the network ‘forgets’ the former inputs. In the image below, the shading indicates the sensitivity of unfolded network, lighter and more vanish.</p>

<p><img src="/assets/images/unfold_lstm.png" width="40%" /></p>

<p>(I’ll make a explanation in the aspect of mathematics in the future)</p>

<p><strong><em>what is LSTM memory block look like?</em></strong></p>

<p>The memory block include some gates and activation function which seted for controlling the gates, in the image below states it.</p>

<p><img src="/assets/images/memory_block_lstm.png" width="40%" /></p>

<p>The tiny black balls represents 3 gates for collecting activation from inside and outside the “<em>memory block</em>” with non-linear activation, and theres has some tings we need focus on:</p>

<ul>
  <li>No activation function is applied within the cell</li>
  <li>$\int_f$ is usually the logistic function, so that it can control the state of gate (open := 1 and close := 0)</li>
  <li>$\int_s \&amp; \int_h$ are usually tanh or logistic function, though in some cases ‘h’ is the identity function</li>
  <li>The weighted ‘peephole’ connections from the cell to the gates are shown with dashed lines. All other connections within the block are unweighted (or equivalently, have a fixed weight of 1.0).</li>
</ul>

<p><strong><em>How LSTM make preservation of gradient information?</em></strong></p>

<p>Suppose the shading of the nodes indicates their sensitivity to the input at time one, so that black maximum while white are entirely insensitive, and the state of input, forget and output gates are displayed below, left and above the hidden layer.</p>

<p><img src="/assets/images/transfer_state.png" width="40%" /></p>

<p><img src="/assets/images/three_state.png" width="40%" /></p>

<p>For simplicity, all gates are either entirely open (‘O’) or closed (‘—’). The memory cell ‘remembers’ the first input as long as the forget gate is open and the input gate is closed. The sensitivity of the output layer can be switched on and off by the output gate without affecting the cell.</p>

<p><strong><em>Maybe LSTM is not the best answer</em></strong></p>

<p>The above discussion raises an important point about the influence of preprocessing. If we can find  a way to transform a task containing long range contextual dependencies into one containing only short-range dependencies before presenting it to a sequence learning algorithm, then architectures such as LSTM become somewhat redundant. Suppose we have a raw speech sample whose frequency is 40kHz, it is obviously that this speech sample is a <em>long range contextual</em> sample. If we can find a way to transform this sample to 100Hz series of mel-frequency cepstral coefficients, it becomes feasibel to model the data with hidden Markov model. Nonetheless, if such a transform is difficult or unknown, or if we simply
wish to get a good result without having to design task-specific preprocessing methods, algorithms capable of handling long time dependencies are essential.</p>

  </section>

  <div class="post-meta">
      <span class="post-meta__tags tags"><i class="fa fa-tags"></i> <a href="/tags/machine-learning/">Machine Learning</a>, <a href="/tags/machine-learning/">Machine Learning</a></span>
    </div>
</article>

<section class="read-more">
   
   
   <div class="read-more-item">
       <span class="read-more-item-dim">最近的文章</span>
       <h2 class="post-list__post-title post-title"><a href="/2017/10/%E5%85%B3%E4%BA%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/" title="link to 关于强化学习的一些简单介绍">关于强化学习的一些简单介绍</a></h2>
       <p class="excerpt">什么是强化学习强化学习是非监督学习的一种，我们可以结合下面这个事实例子来理解，它涉及到这么几个子问题：如何做——如何将情境映射到动作——以便最大化数字化的奖励信号。其实从上面列出的三个子问题，我们可以进一步说明强化学习其实是一个闭环问题，因为学习系统的行为将来又会影响到后面（下一轮学习）的输入此外，学习系统中的“学习者”没有被告知应该采取哪种动作来到达下一状态，而是通过reward的反馈形式来尝试哪些动作或者行为能够让其获得更多的reward。这样的学习机制也反映出强化学习其实是异步的，或...&hellip;</p>
       <div class="post-list__meta"><time datetime="2017-10-03 20:58:00 +0800" class="post-list__meta--date date">2017-10-03</time> <a class="btn-border-small" href=/2017/10/%E5%85%B3%E4%BA%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/>继续阅读</a></div>
   </div>
   
   
   
   
   <div class="read-more-item">
       <span class="read-more-item-dim">更早的文章</span>
       <h2 class="post-list__post-title post-title"><a href="/2017/08/%E4%B8%80%E4%BB%BD%E8%B7%A8%E6%B5%8F%E8%A7%88%E5%99%A8JavaScript%E4%BA%8B%E4%BB%B6%E5%A4%84%E7%90%86%E4%BB%A3%E7%A0%81/" title="link to 一份跨浏览器JavaScript事件处理代码">一份跨浏览器JavaScript事件处理代码</a></h2>
       <p class="excerpt">javascriptvar EventUtil = {    addHandler: function(element, type, handler) {        if (element.addEventListener) {            element.addEventListener(type, handler, false);  // toggle when propagation occur        } else if (element.attchEvent)...&hellip;</p>
       <div class="post-list__meta"><time datetime="2017-08-27 19:51:00 +0800" class="post-list__meta--date date">2017-08-27</time><a class="btn-border-small" href=/2017/08/%E4%B8%80%E4%BB%BD%E8%B7%A8%E6%B5%8F%E8%A7%88%E5%99%A8JavaScript%E4%BA%8B%E4%BB%B6%E5%A4%84%E7%90%86%E4%BB%A3%E7%A0%81/>继续阅读</a></div>
   </div>
   
</section>

<section class="post-comments">
  
    <div id="disqus_thread"></div>
    <script>
    
    var disqus_config = function () {
        this.page.url = "http://kornbergfresnel.github.io/2017/10/LSTM-Tutorial/";
        this.page.identifier = "/2017/10/LSTM-Tutorial/";
    };

    var disqus_shortname = 'vno-jekyll';
    
    (function() { // DON'T EDIT BELOW THIS LINE
        var d = document, s = d.createElement('script');
        s.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();
    </script>
    <noscript>要查看<a href="http://disqus.com/?ref_noscript"> Disqus </a>评论，请启用 JavaScript</noscript>
    
  
  
  
  
</section>


            <section class="footer">
    <footer>
    	<span class="footer__copyright">本站点采用<a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">知识共享 署名-非商业性使用-相同方式共享 4.0 国际 许可协议</a></span>
        <span class="footer__copyright">由 <a href="https://jekyllrb.com">Jekyll</a> 于 2018-01-06 生成，感谢 <a href="https://www.digitalocean.com/?refcode=30ed2d146762">Digital Ocean</a> 为本站提供稳定的 VPS 服务</span>
        <span class="footer__copyright">本站由 <a href="http://twitter/andrew_kornberg">@andrew_kornberg</a> 创建，采用 <a href="https://github.com/onevcat/vno-jekyll">Vno - Jekyll</a> 作为主题，您可以在 GitHub 找到<a href="https://github.com/onevcat/OneV-s-Den">本站源码</a> - &copy; 2018</span>
    </footer>
</section>

        </div>
    </div>
    
    <script type="text/javascript" src="//code.jquery.com/jquery-1.11.3.min.js"></script>

<script type="text/javascript" src="/js/main.js"></script>



    
  </body>

</html>
