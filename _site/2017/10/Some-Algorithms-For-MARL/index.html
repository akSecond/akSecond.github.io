<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <title>Some Algorithms For MARL</title>
  <meta name="description" content="Deep Repeated Update Q-Network">
  <meta name="author" content="Wei Wang">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Some Algorithms For MARL">
  <meta name="twitter:description" content="Deep Repeated Update Q-Network">
  
  <meta property="og:type" content="article">
  <meta property="og:title" content="Some Algorithms For MARL">
  <meta property="og:description" content="Deep Repeated Update Q-Network">
  
  <link rel="icon" type="image/png" href="/assets/images/favicon.png" />
  <link href="/assets/images/favicon.png" rel="shortcut icon" type="image/png">
  
  <link rel="stylesheet" href="/css/main.css">
  <link href="//netdna.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">

  <link rel="canonical" href="http://kornbergfresnel.github.io/2017/10/Some-Algorithms-For-MARL/">
  <link rel="alternate" type="application/rss+xml" title="Andrew Kornberg" href="http://kornbergfresnel.github.io/feed.xml">
  
  <meta name="google-site-verification" content="1-1ZlHoRvM0T2FqPbW2S-qLgYXN6rsn52kErlMPd_gw" />

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>  
  
</head>


  <body>

    <span class="mobile btn-mobile-menu">
        <i class="fa fa-list btn-mobile-menu__icon"></i>
        <i class="fa fa-angle-up btn-mobile-close__icon hidden"></i>
    </span>
    
    <header class="panel-cover panel-cover--collapsed" style="background-image: url('/assets/images/background-cover.jpg')">
  <div class="panel-main">

    <div class="panel-main__inner panel-inverted">
    <div class="panel-main__content">

        <a href="/#blog" title="前往 Andrew Kornberg 的主页" class="blog-button"><img src="/assets/images/avatar.jpg" width="80" alt="Andrew Kornberg logo" class="panel-cover__logo logo" /></a>
        <h1 class="panel-cover__title panel-title"><a href="/#blog" title="link to homepage for Andrew Kornberg" class="blog-button">Andrew Kornberg</a></h1>
        
        <span class="panel-cover__subtitle panel-subtitle">abeunt studia in morse</span>
        
        <hr class="panel-cover__divider" />
        <p class="panel-cover__description">Hi, I'm Kornberg, a wild programmer & PhD. candidate</p>
        <hr class="panel-cover__divider panel-cover__divider--secondary" />
        
        
        
        <div class="navigation-wrapper">
          <div>
            <nav class="cover-navigation cover-navigation--primary">
              <ul class="navigation">
                <li class="navigation__item"><a href="/#blog" title="Visit blog" class="blog-button">Blog</a></li>
                
                  <li class="navigation__item"><a href="http://kornbergfresnel.github.io" target="_blank" title="My open-source projects">Projects</a></li>
                
              </ul>
            </nav>
          </div>
          
          <div><nav class="cover-navigation navigation--social">
  <ul class="navigation">

  
  <!-- Weibo -->
  <li class="navigation__item">
    <a href="http://weibo.com/u/5104230793" title="@u/5104230793 的微博" target="_blank">
      <i class='social fa fa-weibo'></i>
      <span class="label">Weibo</span>
    </a>
  </li>
  

  
  <!-- Github -->
  <li class="navigation__item">
    <a href="https://github.com/KornbergFresnel" title="@KornbergFresnel 的 Github" target="_blank">
      <i class='social fa fa-github'></i>
      <span class="label">Github</span>
    </a>
  </li>
  
  
  
  <!-- Twitter -->
  <li class="navigation__item">
    <a href="http://twitter.com/andrew_kornberg" title="@andrew_kornberg" target="_blank">
      <i class='social fa fa-twitter'></i>
      <span class="label">Twitter</span>
    </a>
  </li>
  

  
  <!-- Google Plus -->
  <li class="navigation__item">
    <a href="https://plus.google.com/107108267983477358170" rel="author" title="Google+" target="_blank">
      <i class='social fa fa-google-plus-square'></i>
      <span class="label">Google Plus</span>
    </a>
  </li>
  

  <!-- RSS -->
  <li class="navigation__item">
    <a href="/feed.xml" rel="author" title="RSS" target="_blank">
      <i class='social fa fa-rss'></i>
      <span class="label">RSS</span>
    </a>
  </li>

  
  <!-- Email -->
  <li class="navigation__item">
    <a href="mailto:kornbergfresnel@outlook.com" title="Contact me">
      <i class='social fa fa-envelope'></i>
      <span class="label">Email</span>
    </a>
  </li>
  

  </ul>
</nav>
</div>
        </div>
      </div>
    </div>
    
    
    <div class="panel-cover--overlay cover-blue"></div>
    
  </div>
</header>


    <div class="content-wrapper">
        <div class="content-wrapper__inner">
            

<article class="post-container post-container--single" itemscope itemtype="http://schema.org/BlogPosting">
  <header class="post-header">
    <div class="post-meta">
      <time datetime="2017-10-27 19:15:00 +0800" itemprop="datePublished" class="post-meta__date date">2017-10-27</time> &#8226; <span class="post-meta__tags tags"><i class="fa fa-tags"></i> <a href="/tags/machine-learning/">Machine Learning</a>, <a href="/tags/reinforcement-learning/">Reinforcement Learning</a>, <a href="/tags/reinforcement-learning/">Reinforcement Learning</a></span>
    </div>
    <h1 class="post-title">Some Algorithms For MARL</h1>
  </header>

  <section class="post">
    <p><strong>Deep Repeated Update Q-Network</strong></p>

<p>It tries to address an issue in the way Q-learning estimate the value of an action. Ideally, if an agent could execute every possible action in parallel but identical environments at each time step, then information about all possible actions could be gathered in order to update every action value simultaneously. From this conjecture, RUQL proposes that an <strong>action value must be updated inversely proportional to the probability of the action selected</strong> given the policy that is being followed. Thus when an action with low probability is selected, the corresponding action-value is updated more than once. By contrast, if an action with high probability is selected, then the action-value may be updated only once. Algorithm 5 provides an initial way to formalize this intuition, while this algorithm may become unbounded in computation time as $\pi(s, a) \to 0$.</p>

<p><img src="/assets/images/ruql.png" width="45%" /></p>

<p><img src="/assets/images/ruql2.png" width="45%" /></p>

<p>The implement of alrorithm-6 states that if an action has a very high chance of being selected then $1 \over {\pi(s, a)} \to 1$ and standard Q-Learning is recovered. On the other hand when an action is rarely selected then not only the action-value is updated inversely proportional but also the new estimates carry more weight.</p>

<p><strong>Deep Loosely Coupled Q-Network</strong></p>

<p>Assumes that an agent is not capable of observing the full information content of the environment. Therefore an agent has to learn under which circumstances it has to act independently adn when in coordination with other agents or the information they provide. This alrogithm makes explicit considerations about multi-agents.</p>

<p><em>agent independence</em></p>

<p>An independence degree $\epsilon^k_i \in [0,1]$ for agent $i$ in state $s^k_i$ determines the probability of an agent carrying on an action independently. The closer $\epsilon^k_i$ is to the upper bound, the more certainty there is for an agent to act based on its individual information regradless of the presence of other agents. We determining independence degree with the negative outcomes it receives, many methods you can use, such as Gaussian-like diffusion distribution.</p>

<p><strong>Decentralized Markove Decision Processes</strong></p>

<p>In multi-agent domains, an agent may not only depend on the information it has gathered about its environment. It will also be influenced by the choices of other agents. Naturally, these problems are partially observable. Decentralized Partially Observable Markov Decision Processes (Dec-POMDP) (Bernstein et al., 2000) have been developed as an extension of POMDPs to address situations where agents can exploit levels of coordination among them.</p>

<p><strong>Overoptimistic estimation in Q-Learning</strong></p>

<p>Double Q-network, the study addresses it by decoupling the selection and the evaluation of actions. In Sorokin et al.(2015), they extend DQN to LSTM networks to present areas of attention. And other work extends beyond the application of deep neural networks to Q-learning as it is the case in Lillicrap et al.(2015), where they present an algorithm that generalizes to continuous spaces using deterministic policy gradients.</p>

<p><strong>Multi-Agent Reinforcement Learning</strong></p>

<p>As we have seen, reinforcement learning provides an alternative to deal with the constantly changing environments. RL agents learn from experience by observing their environment and the effect of their actions. Nonetheless the transition from single agent RL to multi-agent RL offers a series challenges.</p>

<p>The reward that the agent may receive will not only depend on its interaction with a passive environment, In multi-agent environments, it is intertwined with the actions made by the others. Defining a goal becomes complex because the rewards are correlated and cannot simply be maximized independently, cause it should concern the global environment.</p>

<p>One of the biggest open issues in multi-agent environments is how to deal with non-stationarity. A policy is optimal and stationary when it is the best possible policy and it remains fixed over time. Due to the dependence of the reward function on the actions taken by other agents, good policies at a given point could not be so in the future. They are only good policies in relation to what the other agents have learned at the time the policy is applied. The exploration-exploitation dilemma becomes even more relevant under these settings. Information gathering is not only important initially but has to be done with certain recurrence while at the same time being careful that it does not destabilize the agent or agents when an appropriate coordination is required.</p>

<p>In practice, convergence in most complex multi-agent problems tends to be empirically verified. In some cases single RL algorithms such as Q-Learning have been used with no modification (Claus and Boutilier, 1998; Crites and Barto, 1998; Tan, 1993). However several extensions to a multi-agent domain have been proposed for cooperative tasks (Kapetanakis and Kudenko, 2005; Lauer and Riedmiller, 2000; Littman, 2001b), competitive tasks (Littman, 1994) as well as mixed tasks (Tesauro, 2003). There has two extensions to Q-Learning are presented. Each of them tries to address a concern or weakness of Q-Learning when dealing with multi-agent or non-stationary tasks. These two algorithms will serve as the basis of novel extensions to large state spaces.</p>


  </section>

  <div class="post-meta">
      <span class="post-meta__tags tags"><i class="fa fa-tags"></i> <a href="/tags/machine-learning/">Machine Learning</a>, <a href="/tags/reinforcement-learning/">Reinforcement Learning</a>, <a href="/tags/reinforcement-learning/">Reinforcement Learning</a></span>
    </div>
</article>

<section class="read-more">
   
   
   <div class="read-more-item">
       <span class="read-more-item-dim">最近的文章</span>
       <h2 class="post-list__post-title post-title"><a href="/2017/11/SILENCE/" title="link to SILENCE">SILENCE</a></h2>
       <p class="excerpt">​​Yeah, I'd rather be a lover than a fighter​Cause all my life, I've been fighting​Never felt a feeling of comfort​All this time, I've been hiding ​And I never had someone to call my own, oh nah​I'm so used to sharing​Love only left me alone​But I...&hellip;</p>
       <div class="post-list__meta"><time datetime="2017-11-02 23:30:00 +0800" class="post-list__meta--date date">2017-11-02</time> <a class="btn-border-small" href=/2017/11/SILENCE/>继续阅读</a></div>
   </div>
   
   
   
   
   <div class="read-more-item">
       <span class="read-more-item-dim">更早的文章</span>
       <h2 class="post-list__post-title post-title"><a href="/2017/10/Monte-Carlo-Methods/" title="link to Monte Carlo Methods">Monte Carlo Methods</a></h2>
       <p class="excerpt">Off-policy Prediction via Importance SamplingTo be continued…Off-policy Monte Carlo ControlIn off-policy method, estimating the value of a policy and controling are separated, it is the mainly difference between on-policy and off-policy. In off-po...&hellip;</p>
       <div class="post-list__meta"><time datetime="2017-10-27 12:10:00 +0800" class="post-list__meta--date date">2017-10-27</time><a class="btn-border-small" href=/2017/10/Monte-Carlo-Methods/>继续阅读</a></div>
   </div>
   
</section>

<section class="post-comments">
  
    <div id="disqus_thread"></div>
    <script>
    
    var disqus_config = function () {
        this.page.url = "http://kornbergfresnel.github.io/2017/10/Some-Algorithms-For-MARL/";
        this.page.identifier = "/2017/10/Some-Algorithms-For-MARL/";
    };

    var disqus_shortname = 'vno-jekyll';
    
    (function() { // DON'T EDIT BELOW THIS LINE
        var d = document, s = d.createElement('script');
        s.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();
    </script>
    <noscript>要查看<a href="http://disqus.com/?ref_noscript"> Disqus </a>评论，请启用 JavaScript</noscript>
    
  
  
  
  
</section>


            <section class="footer">
    <footer>
    	<span class="footer__copyright">本站点采用<a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">知识共享 署名-非商业性使用-相同方式共享 4.0 国际 许可协议</a></span>
        <span class="footer__copyright">由 <a href="https://jekyllrb.com">Jekyll</a> 于 2018-01-06 生成，感谢 <a href="https://www.digitalocean.com/?refcode=30ed2d146762">Digital Ocean</a> 为本站提供稳定的 VPS 服务</span>
        <span class="footer__copyright">本站由 <a href="http://twitter/andrew_kornberg">@andrew_kornberg</a> 创建，采用 <a href="https://github.com/onevcat/vno-jekyll">Vno - Jekyll</a> 作为主题，您可以在 GitHub 找到<a href="https://github.com/onevcat/OneV-s-Den">本站源码</a> - &copy; 2018</span>
    </footer>
</section>

        </div>
    </div>
    
    <script type="text/javascript" src="//code.jquery.com/jquery-1.11.3.min.js"></script>

<script type="text/javascript" src="/js/main.js"></script>



    
  </body>

</html>
