<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <title>关于强化学习的一些简单介绍</title>
  <meta name="description" content="什么是强化学习">
  <meta name="author" content="Wei Wang">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="关于强化学习的一些简单介绍">
  <meta name="twitter:description" content="什么是强化学习">
  
  <meta property="og:type" content="article">
  <meta property="og:title" content="关于强化学习的一些简单介绍">
  <meta property="og:description" content="什么是强化学习">
  
  <link rel="icon" type="image/png" href="/assets/images/favicon.png" />
  <link href="/assets/images/favicon.png" rel="shortcut icon" type="image/png">
  
  <link rel="stylesheet" href="/css/main.css">
  <link href="//netdna.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">

  <link rel="canonical" href="http://kornbergfresnel.github.io/2017/10/%E5%85%B3%E4%BA%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/">
  <link rel="alternate" type="application/rss+xml" title="Andrew Kornberg" href="http://kornbergfresnel.github.io/feed.xml">
  
  <meta name="google-site-verification" content="1-1ZlHoRvM0T2FqPbW2S-qLgYXN6rsn52kErlMPd_gw" />

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>  
  
</head>


  <body>

    <span class="mobile btn-mobile-menu">
        <i class="fa fa-list btn-mobile-menu__icon"></i>
        <i class="fa fa-angle-up btn-mobile-close__icon hidden"></i>
    </span>
    
    <header class="panel-cover panel-cover--collapsed" style="background-image: url('/assets/images/background-cover.jpg')">
  <div class="panel-main">

    <div class="panel-main__inner panel-inverted">
    <div class="panel-main__content">

        <a href="/#blog" title="前往 Andrew Kornberg 的主页" class="blog-button"><img src="/assets/images/avatar.jpg" width="80" alt="Andrew Kornberg logo" class="panel-cover__logo logo" /></a>
        <h1 class="panel-cover__title panel-title"><a href="/#blog" title="link to homepage for Andrew Kornberg" class="blog-button">Andrew Kornberg</a></h1>
        
        <span class="panel-cover__subtitle panel-subtitle">abeunt studia in morse</span>
        
        <hr class="panel-cover__divider" />
        <p class="panel-cover__description">Hi, I'm Kornberg, a wild programmer & PhD. candidate</p>
        <hr class="panel-cover__divider panel-cover__divider--secondary" />
        
        
        
        <div class="navigation-wrapper">
          <div>
            <nav class="cover-navigation cover-navigation--primary">
              <ul class="navigation">
                <li class="navigation__item"><a href="/#blog" title="Visit blog" class="blog-button">Blog</a></li>
                
                  <li class="navigation__item"><a href="http://kornbergfresnel.github.io" target="_blank" title="My open-source projects">Projects</a></li>
                
              </ul>
            </nav>
          </div>
          
          <div><nav class="cover-navigation navigation--social">
  <ul class="navigation">

  
  <!-- Weibo -->
  <li class="navigation__item">
    <a href="http://weibo.com/u/5104230793" title="@u/5104230793 的微博" target="_blank">
      <i class='social fa fa-weibo'></i>
      <span class="label">Weibo</span>
    </a>
  </li>
  

  
  <!-- Github -->
  <li class="navigation__item">
    <a href="https://github.com/KornbergFresnel" title="@KornbergFresnel 的 Github" target="_blank">
      <i class='social fa fa-github'></i>
      <span class="label">Github</span>
    </a>
  </li>
  
  
  
  <!-- Twitter -->
  <li class="navigation__item">
    <a href="http://twitter.com/andrew_kornberg" title="@andrew_kornberg" target="_blank">
      <i class='social fa fa-twitter'></i>
      <span class="label">Twitter</span>
    </a>
  </li>
  

  
  <!-- Google Plus -->
  <li class="navigation__item">
    <a href="https://plus.google.com/107108267983477358170" rel="author" title="Google+" target="_blank">
      <i class='social fa fa-google-plus-square'></i>
      <span class="label">Google Plus</span>
    </a>
  </li>
  

  <!-- RSS -->
  <li class="navigation__item">
    <a href="/feed.xml" rel="author" title="RSS" target="_blank">
      <i class='social fa fa-rss'></i>
      <span class="label">RSS</span>
    </a>
  </li>

  
  <!-- Email -->
  <li class="navigation__item">
    <a href="mailto:kornbergfresnel@outlook.com" title="Contact me">
      <i class='social fa fa-envelope'></i>
      <span class="label">Email</span>
    </a>
  </li>
  

  </ul>
</nav>
</div>
        </div>
      </div>
    </div>
    
    
    <div class="panel-cover--overlay cover-blue"></div>
    
  </div>
</header>


    <div class="content-wrapper">
        <div class="content-wrapper__inner">
            

<article class="post-container post-container--single" itemscope itemtype="http://schema.org/BlogPosting">
  <header class="post-header">
    <div class="post-meta">
      <time datetime="2017-10-03 20:58:00 +0800" itemprop="datePublished" class="post-meta__date date">2017-10-03</time> &#8226; <span class="post-meta__tags tags"><i class="fa fa-tags"></i> <a href="/tags/machine-learning/">Machine Learning</a>, <a href="/tags/reinforcement-learning/">Reinforcement Learning</a>, <a href="/tags/sutton-book/">Sutton Book</a></span>
    </div>
    <h1 class="post-title">关于强化学习的一些简单介绍</h1>
  </header>

  <section class="post">
    <h2 id="section">什么是强化学习</h2>

<p>强化学习是非监督学习的一种，我们可以结合下面这个事实例子来理解，它涉及到这么几个子问题：如何做——如何将情境映射到动作——以便最大化数字化的奖励信号。</p>

<p>其实从上面列出的三个子问题，我们可以进一步说明强化学习其实是一个闭环问题，因为学习系统的行为将来又会影响到后面（下一轮学习）的输入</p>

<p><img src="/assets/images/sub_prob.png" width="40%" /></p>

<p>此外，学习系统中的“学习者”没有被告知应该采取哪种动作来到达下一状态，而是通过reward的反馈形式来尝试哪些动作或者行为能够让其获得更多的reward。这样的学习机制也反映出强化学习其实是异步的，或者说它的学习过程是有延迟的。它不能够及时的根据当前的境况进行调整，reward的反馈只有在learner完成了一轮学习任务之后才能够被接收，同时反映到下一轮的决策中来。</p>

<p>强化学习相当于是基于过往的学习经验而形成最终的行为的一个学习模型。它的一个特征是在整个学习过程中需要考虑学习和实践的平衡。另一个关键特征是，它明确地描述了一个面向目标的learner与不确定环境交互的整个问题，比如当强化学习应用在游戏中时，玩家的目标很明确，就是比如得到更多的分数。这与许多考虑子问题的方法形成对比。</p>

<h2 id="section-1">强化学习的几个组成元素</h2>

<p><strong>Policy</strong></p>

<p>policy定义了学习系统中agent（或者称学习者）在给定时间内的行为方式，粗略地讲，一个policy是一个从预先定义好的环境state集合到动作的映射。它对应于心理学中被称为一组刺激反应规则或关联（刺激包括可能来自动物自身的刺激）。</p>

<p>policy是强化学习中的核心，因为只有这样才能够确定行为。 一般来说，policy可能是随机的。</p>

<p><strong>Reward</strong></p>

<p>reward在整体上能够定义一个强化学习问题，因为agent能够通过reward的值的反馈来对周围环境做出一个了解，同时引导agent往其学习目标不断发展优化。比如在一个生物系统中，我们可能将奖励视为类似于快乐或痛苦的经历。</p>

<p><strong>Value function</strong></p>

<p>reward直接意义上告诉agent是什么好什么是坏，而值函数则告诉agent在其整个长期的学习中什么是好什么是坏。粗略地讲，当前状态的价值就是从当前这个状态开始以及在后续学习过程中积累的reward的值是多少。</p>

<script type="math/tex; mode=display">V(s) \leftarrow \sum_{i=s}^nR_i</script>

<p><strong>Model</strong></p>

<p>model起到一个模仿真实环境的作用，或更一般地，可以推断出环境的行为。 例如，给定状态和动作，模型可以预测下一个状态和下一个奖励。</p>

<p>模型也用于规划，通过这些模型的帮助，能够在实际经历之前考虑未来可能的情况来决定未来的行动过程。使用模型和规划来解决强化学习问题的方法被称为基于模型的方法，而不是简单的无模型的方法，这些方法属于明确的试错学习，被视为几乎与规划相反。 之后会探讨通过反复尝试同时学习的强化学习系统，学习环境模型，并使用该模型进行规划。</p>

<h2 id="section-2">演化算法和强化学习算法</h2>

<p>一些搜索方法，比如genetic algorithms, genetic programming, simulated annealing，以及其他的一些，都曾经被用来解决强化学习问题。他们的共同点是直接在policy空间搜索而不追求价值函数$Q$的体现，我们也把这样的搜索方法称为演化算法，因为这些算法的执行方式可以类比为生物进化。就像生物进化出具有特定行为的器官而不需要它们在有限的生命时间中进行学习。如果policy空间足够小，或者可以结构化，使得好的policy是常见的或容易找到的，那么进化方法就可以有效。此外，进化方法对于学习者无法准确感知其环境状态的问题具有优势。</p>

<p>然而，强化学习的意思是涉及在与环境相互作用的同时进行学习，但这些演化方法不能做。我们认为在许多情况下，能够利用个体行为交互细节的方法比演化算法更有效率。</p>

<p><strong>演化算法的一些缺点</strong>：演化算法一般都忽视了强化学习问题中的一些有用的结构，比如不考虑它们搜索的那些policy其实是一些关于状态到动作的函数映射，也不注意某个学习个体在其整个学习过程中，动作和状态的选择过程。</p>

<h2 id="section-3">强化学习的历史发展</h2>

<p>强化学习的历史发展分成两条路线，但如今这两条发展线路已经纽结为当今你所看见的强化学习。一个发展方向是通过试错学习（<em>trail and error</em>）并且主要应用在动物学习心理学上（<em>psychology of animal learning</em>），也正是这样的一些早期研究催生了强化学习。另一个发展方向关注的是优化控制（<em>optimal control</em>）并且它采取的解决方案是结合值函数和动态规划。</p>

<h3 id="section-4">优化控制</h3>

<p>优化控制这个概念是在1950末期提出，并用来描述“设计一个能够最小化动态系统测量行为”的问题。其实就是Bellman提出的动态规划理论，Bellman本人也介绍了另一种离散随机版本的优化控制问题，即马尔可夫决策过程（<em>Markovian decision processes</em>），然后Ron Howard为MDPs设计了一种决策迭代方法。以上这些奠定了现代强化学习的算法和理论基础。</p>

<p>即使传统的动态规划方法呈现一种计算昂贵，但是相比于同时代的其他方法，这已经是最好的了，所以这一不难理解，在过去的几十年，动态规划在强化学习的很多相关领域都备受青睐。</p>

<h3 id="section-5">试错学习</h3>

<p>我们回到历史发展的另一条线，试错学习。这种方法最早出现在心理学研究中，和学习中的“增强”理论有一些共同点，所以被考虑到强化学习研究中。在心理学研究中，和“增强”理论有对应的是被称为效应法则（<em>Law of Effect</em>）的心理学理论。</p>

<p><strong>选择性（<em>selectional</em>）和联想能力（<em>associative</em>）</strong></p>

<p>效应法则包含试错学习利用的两个重要方面，一个是选择性（<em>selectional</em>），一个是联想（<em>associative</em>）。监督学习具有联想能力，但不具有选择性，而试错学习和效应法则包含这两种特性的结合。我们可以这样理解：试错学习是一种简单的搜索和记忆方法，通过选择和尝试的方式进行搜索和记忆，以记忆哪些行为最有效的方式进行记忆，然后将他们联系起来。</p>

<p>Minsky在他的博士论文中提出了一种强化学习计算模型，并描述了他构建的模拟机器，他称之为SNARCs（随机神经 - 模拟增强计算器，<em>Stochastic Neural-Analog Reinforcement Calculators</em>）组成，这也是强化学习模型的第一次出现。之后Farley和Clark描述了另一种被设计用来进行试错学习的神经网络学习机器，而有趣的是，这两人在后来又从试错学习的研究转向泛化研究和模式识别问题中去了，也就是从强化学习阵营转向监督学习。这样的研究方向转变竟然导致这些研究话题之间的关系模糊化，很多研究者似乎开始相信他们在进行的强化学习研究工作实际的落脚点在监督学习。也正因为这样的意识上的模糊，导致在1960-1970年代之间实际关于试错学习的研究非常少。</p>

<p>在1960-1970年之间，有显著成果的应该算是John Andreae和Donald Michie。前者在1963年提出STeLLA系统，一个通过试错学习与环境进行交互的系统，该系统包含一个内部世界模型以及一个处理隐藏状态问题的agent；后者作出的工作更有影响力，在1961和1963年，他描述了一个称为MENACE的用来完成tic-tac-toe游戏的简单试错学习系统。</p>

<p><img src="/assets/images/tic_toe.png" width="40%" /></p>

<h3 id="temporal-difference-learning">时间差异学习（<em>temporal-difference learning</em>）</h3>

<p>Temporal-difference learning methods are distinctive in being driven by the difference between temporally successive estimates of the same quantity—for example, of the probability of winning in the tic-tac-toe example. This thread is smaller and less distinct than the other two, but it has played a particularly important role in the field, in part because temporal-difference methods seem to be new and unique to reinforcement learning.</p>


  </section>

  <div class="post-meta">
      <span class="post-meta__tags tags"><i class="fa fa-tags"></i> <a href="/tags/machine-learning/">Machine Learning</a>, <a href="/tags/reinforcement-learning/">Reinforcement Learning</a>, <a href="/tags/sutton-book/">Sutton Book</a></span>
    </div>
</article>

<section class="read-more">
   
   
   <div class="read-more-item">
       <span class="read-more-item-dim">最近的文章</span>
       <h2 class="post-list__post-title post-title"><a href="/2017/10/Multi-armed-Bandits/" title="link to Multi-armed Bandits">Multi-armed Bandits</a></h2>
       <p class="excerpt">  将强化学习和其他学习方式区分开来的一个重要特征是：RL通过评估选取动作而不是指导agent应该执行哪些正确的动作，但其实将这两种方式融合在一起也是很有趣的。$k$-armed Bandit问题比如你需要在 $k$ 个不同的action中重复做出选择，在每个action决定作出后，你都会得到服从固定概率分布的reward，你的目标是最大化整个学习过程中的reward。为了方便后面的描述，我们做出如下定义：  $A_t$: the action selected on time step ...&hellip;</p>
       <div class="post-list__meta"><time datetime="2017-10-07 22:44:00 +0800" class="post-list__meta--date date">2017-10-07</time> <a class="btn-border-small" href=/2017/10/Multi-armed-Bandits/>继续阅读</a></div>
   </div>
   
   
   
   
   <div class="read-more-item">
       <span class="read-more-item-dim">更早的文章</span>
       <h2 class="post-list__post-title post-title"><a href="/2017/10/LSTM-Tutorial/" title="link to LSTM Tutorial">LSTM Tutorial</a></h2>
       <p class="excerpt">  the LSTM network includes a set of recurrently connected subnets, which we call as “memory block”, these memory block are made with one or more self-conncted memory cells and three multiplicative units—the input, output, and forget gates—that pr...&hellip;</p>
       <div class="post-list__meta"><time datetime="2017-10-02 16:41:00 +0800" class="post-list__meta--date date">2017-10-02</time><a class="btn-border-small" href=/2017/10/LSTM-Tutorial/>继续阅读</a></div>
   </div>
   
</section>

<section class="post-comments">
  
    <div id="disqus_thread"></div>
    <script>
    
    var disqus_config = function () {
        this.page.url = "http://kornbergfresnel.github.io/2017/10/%E5%85%B3%E4%BA%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/";
        this.page.identifier = "/2017/10/%E5%85%B3%E4%BA%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/";
    };

    var disqus_shortname = 'vno-jekyll';
    
    (function() { // DON'T EDIT BELOW THIS LINE
        var d = document, s = d.createElement('script');
        s.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();
    </script>
    <noscript>要查看<a href="http://disqus.com/?ref_noscript"> Disqus </a>评论，请启用 JavaScript</noscript>
    
  
  
  
  
</section>


            <section class="footer">
    <footer>
    	<span class="footer__copyright">本站点采用<a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">知识共享 署名-非商业性使用-相同方式共享 4.0 国际 许可协议</a></span>
        <span class="footer__copyright">由 <a href="https://jekyllrb.com">Jekyll</a> 于 2018-01-06 生成，感谢 <a href="https://www.digitalocean.com/?refcode=30ed2d146762">Digital Ocean</a> 为本站提供稳定的 VPS 服务</span>
        <span class="footer__copyright">本站由 <a href="http://twitter/andrew_kornberg">@andrew_kornberg</a> 创建，采用 <a href="https://github.com/onevcat/vno-jekyll">Vno - Jekyll</a> 作为主题，您可以在 GitHub 找到<a href="https://github.com/onevcat/OneV-s-Den">本站源码</a> - &copy; 2018</span>
    </footer>
</section>

        </div>
    </div>
    
    <script type="text/javascript" src="//code.jquery.com/jquery-1.11.3.min.js"></script>

<script type="text/javascript" src="/js/main.js"></script>



    
  </body>

</html>
