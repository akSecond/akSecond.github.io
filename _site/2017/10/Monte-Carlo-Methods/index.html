<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <title>Monte Carlo Methods</title>
  <meta name="description" content="Off-policy Prediction via Importance Sampling">
  <meta name="author" content="Wei Wang">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Monte Carlo Methods">
  <meta name="twitter:description" content="Off-policy Prediction via Importance Sampling">
  
  <meta property="og:type" content="article">
  <meta property="og:title" content="Monte Carlo Methods">
  <meta property="og:description" content="Off-policy Prediction via Importance Sampling">
  
  <link rel="icon" type="image/png" href="/assets/images/favicon.png" />
  <link href="/assets/images/favicon.png" rel="shortcut icon" type="image/png">
  
  <link rel="stylesheet" href="/css/main.css">
  <link href="//netdna.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">

  <link rel="canonical" href="http://kornbergfresnel.github.io/2017/10/Monte-Carlo-Methods/">
  <link rel="alternate" type="application/rss+xml" title="Andrew Kornberg" href="http://kornbergfresnel.github.io/feed.xml">
  
  <meta name="google-site-verification" content="1-1ZlHoRvM0T2FqPbW2S-qLgYXN6rsn52kErlMPd_gw" />

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>  
  
</head>


  <body>

    <span class="mobile btn-mobile-menu">
        <i class="fa fa-list btn-mobile-menu__icon"></i>
        <i class="fa fa-angle-up btn-mobile-close__icon hidden"></i>
    </span>
    
    <header class="panel-cover panel-cover--collapsed" style="background-image: url('/assets/images/background-cover.jpg')">
  <div class="panel-main">

    <div class="panel-main__inner panel-inverted">
    <div class="panel-main__content">

        <a href="/#blog" title="前往 Andrew Kornberg 的主页" class="blog-button"><img src="/assets/images/avatar.jpg" width="80" alt="Andrew Kornberg logo" class="panel-cover__logo logo" /></a>
        <h1 class="panel-cover__title panel-title"><a href="/#blog" title="link to homepage for Andrew Kornberg" class="blog-button">Andrew Kornberg</a></h1>
        
        <span class="panel-cover__subtitle panel-subtitle">abeunt studia in morse</span>
        
        <hr class="panel-cover__divider" />
        <p class="panel-cover__description">Hi, I'm Kornberg, a wild programmer & PhD. candidate</p>
        <hr class="panel-cover__divider panel-cover__divider--secondary" />
        
        
        
        <div class="navigation-wrapper">
          <div>
            <nav class="cover-navigation cover-navigation--primary">
              <ul class="navigation">
                <li class="navigation__item"><a href="/#blog" title="Visit blog" class="blog-button">Blog</a></li>
                
                  <li class="navigation__item"><a href="http://kornbergfresnel.github.io" target="_blank" title="My open-source projects">Projects</a></li>
                
              </ul>
            </nav>
          </div>
          
          <div><nav class="cover-navigation navigation--social">
  <ul class="navigation">

  
  <!-- Weibo -->
  <li class="navigation__item">
    <a href="http://weibo.com/u/5104230793" title="@u/5104230793 的微博" target="_blank">
      <i class='social fa fa-weibo'></i>
      <span class="label">Weibo</span>
    </a>
  </li>
  

  
  <!-- Github -->
  <li class="navigation__item">
    <a href="https://github.com/KornbergFresnel" title="@KornbergFresnel 的 Github" target="_blank">
      <i class='social fa fa-github'></i>
      <span class="label">Github</span>
    </a>
  </li>
  
  
  
  <!-- Twitter -->
  <li class="navigation__item">
    <a href="http://twitter.com/andrew_kornberg" title="@andrew_kornberg" target="_blank">
      <i class='social fa fa-twitter'></i>
      <span class="label">Twitter</span>
    </a>
  </li>
  

  
  <!-- Google Plus -->
  <li class="navigation__item">
    <a href="https://plus.google.com/107108267983477358170" rel="author" title="Google+" target="_blank">
      <i class='social fa fa-google-plus-square'></i>
      <span class="label">Google Plus</span>
    </a>
  </li>
  

  <!-- RSS -->
  <li class="navigation__item">
    <a href="/feed.xml" rel="author" title="RSS" target="_blank">
      <i class='social fa fa-rss'></i>
      <span class="label">RSS</span>
    </a>
  </li>

  
  <!-- Email -->
  <li class="navigation__item">
    <a href="mailto:kornbergfresnel@outlook.com" title="Contact me">
      <i class='social fa fa-envelope'></i>
      <span class="label">Email</span>
    </a>
  </li>
  

  </ul>
</nav>
</div>
        </div>
      </div>
    </div>
    
    
    <div class="panel-cover--overlay cover-blue"></div>
    
  </div>
</header>


    <div class="content-wrapper">
        <div class="content-wrapper__inner">
            

<article class="post-container post-container--single" itemscope itemtype="http://schema.org/BlogPosting">
  <header class="post-header">
    <div class="post-meta">
      <time datetime="2017-10-27 12:10:00 +0800" itemprop="datePublished" class="post-meta__date date">2017-10-27</time> &#8226; <span class="post-meta__tags tags"><i class="fa fa-tags"></i> <a href="/tags/machine-learning/">Machine Learning</a>, <a href="/tags/reinforcement-learning/">Reinforcement Learning</a>, <a href="/tags/sutton-book/">Sutton Book</a></span>
    </div>
    <h1 class="post-title">Monte Carlo Methods</h1>
  </header>

  <section class="post">
    <h2 id="off-policy-prediction-via-importance-sampling">Off-policy Prediction via Importance Sampling</h2>

<p><em>To be continued…</em></p>

<h2 id="off-policy-monte-carlo-control">Off-policy Monte Carlo Control</h2>

<p>In off-policy method, estimating the value of a policy and controling are separated, it is the mainly difference between on-policy and off-policy. In off-policy, we separate all possible policies into <em>behavior policy</em> and <em>target policy</em>. The later is inclued in the former, while it is the real policy that is evaluated and improved. For gurantee this rule which <em>target policy</em> should has the chance to visit all <em>behavior policy</em>, we need make a promise that the probability of all <em>behavior policy</em> should larger than zero. We define <em>behavior policy</em> as policy $b$, and <em>target policy</em> as policy $\pi$, then the policy $b$ may be $\epsilon$-soft greedy policy.</p>

<p><img src="/assets/images/off_policy_monte_carlo.png" width="40%" /></p>

<p>A potential problem is that this method learns only from the tails of episodes, when all of the remaining actions in the episode are greedy. If nongreedy actions are common, then learning will be slow, particularly for states appearing in the early portions of long episodes. Potentially, this could greatly slow learning. There has been insufficient experience with off-policy Monte Carlo methods to assess how serious this problem is. If it is serious, the most important way to address it is probably by incorporating temporal-difference learning, the algorithmic idea developed in the next chapter. Alternatively, if γ is less than 1, then the idea developed in the next section may also help significantly.</p>

<h2 id="discounting-aware-importance-sampling">Discounting-Aware Importance Sampling</h2>

<p>If we consider discounting reward in a long episode, then there maybe some problem. Suppose discounting index $\gamma=0$, and the length of episode is 100, then if we  use importance sampling as the old form:</p>

<script type="math/tex; mode=display">\rho={ {\pi(A_0 \mid S_0)\pi(A_1 \mid S_1)…\pi(A_{99} \mid S_{99})} \over {b(A_0 \mid S_0)b(A_1 \mid S_1)…b(A_{99} \mid S_{99})} }</script>

<p>While the return from time 0 will then be $G_0=R_1$. In ordinary importance sampling, the return will be scaled by the entire product, but it is really only necessaryto scale by the first factor, by $\rho={ {\pi(A_0 \mid S_0)} \over {b(A_0 \mid S_0)} }$. The other 99 factors are irrelevant. They do not change the expected update, but they add <strong>enormously to its variance</strong>. In some cases they could even make the variance infinite. So, how we can avoid this bad condition?</p>

<p><strong>degree of partial termination</strong></p>

<p>The essence of the idea is to think of discounting as <em>determining a probability of termination</em> or, equivalently, a degree of partial termination. That is if we terminate at the first step, then return $G_0$; if terminate after two steps, then to the degree of $\gamma(1-\gamma)$, producing a return of $R_1 + \gamma R2$. The degree of termination on the third step is thus $(1-\gamma)\gamma^2$, with the $\gamma^2$ reflecting that termination did not occur on either of the first two steps. The partial returns here are called flat partial returns:</p>

<script type="math/tex; mode=display">\overline{G}=R_{t+1} + R_{t+2}+…+R_{h}, 0 \le t \lt h \le T</script>

<p>The conventional full return Gt can be viewed as a sum of flat partial returns as suggested above as
follows:</p>

<p><img src="/assets/images/degree_formula.png" width="50%" /></p>

<p>Then we define the new ordinary importance-sampling and weighted important-sampling as follow:</p>

<p><img src="/assets/images/new_ordinary_formula.png" width="50%" /></p>

<p><img src="/assets/images/new_weighted_formula.png" width="50%" /></p>

<h2 id="per-reward-importance-sampling">Per-Reward Importance Sampling</h2>

<p>As I writed in the former text, in importance sampling, if we concered about only the reward at time $t$, then the $\pi_{t+k} \over b_{t+k}$ after time $t$ are irrelevant. And all the other ratios are independent random variables whose expected value is one:</p>

<script type="math/tex; mode=display">E_{A_k \thicksim b} \lgroup { {\pi(A_k | S_k)} \over {b(A_k | S_k)} } \rgroup=\sum_a{b(a|S_k){ {\pi(a|S_k)} \over {b(a|S_k)} } }=\sum_a{\pi(a|S_k)}=1</script>

<p>so, we can make a conclusion: $E[\rho_{t:T-1}R_{t+1}]=E[\rho_{t:t}R_{t+1}]$. Braodcast to all $R_k$, then the formula of $G_t$ is:</p>

<script type="math/tex; mode=display">\rho_{t:t}R_{t+1}+\gamma\rho_{t:t+1}R_{t+2}+\gamma^2\rho_{t:t+2}R_{t+3}+…+\gamma^{T-t-1}\rho_{t:T-1}R_T</script>

<p>It is also a unbias estimator as the ordinary importance sampling, we named it as <em>pre-reward importance sampling</em>. Is there a per-reward version of weighted importance sampling? This is less clear. So far, all the estimators that have been proposed for this that we know of are not consistent (that is, they do not converge to the true value with infinite data).</p>

<p>​			
​		
​</p>

  </section>

  <div class="post-meta">
      <span class="post-meta__tags tags"><i class="fa fa-tags"></i> <a href="/tags/machine-learning/">Machine Learning</a>, <a href="/tags/reinforcement-learning/">Reinforcement Learning</a>, <a href="/tags/sutton-book/">Sutton Book</a></span>
    </div>
</article>

<section class="read-more">
   
   
   <div class="read-more-item">
       <span class="read-more-item-dim">最近的文章</span>
       <h2 class="post-list__post-title post-title"><a href="/2017/10/Some-Algorithms-For-MARL/" title="link to Some Algorithms For MARL">Some Algorithms For MARL</a></h2>
       <p class="excerpt">Deep Repeated Update Q-NetworkIt tries to address an issue in the way Q-learning estimate the value of an action. Ideally, if an agent could execute every possible action in parallel but identical environments at each time step, then information a...&hellip;</p>
       <div class="post-list__meta"><time datetime="2017-10-27 19:15:00 +0800" class="post-list__meta--date date">2017-10-27</time> <a class="btn-border-small" href=/2017/10/Some-Algorithms-For-MARL/>继续阅读</a></div>
   </div>
   
   
   
   
   <div class="read-more-item">
       <span class="read-more-item-dim">更早的文章</span>
       <h2 class="post-list__post-title post-title"><a href="/2017/10/Multi-Step-Bootstrapping/" title="link to Multi-Step Boostrapping">Multi-Step Boostrapping</a></h2>
       <p class="excerpt">N-Step TD PredictionAn important property of n-step returns is that their expectation is guranteed to be a better estimate of $v_{\pi}$ than $V_{t+n-1}$ is, in a worst-state sense. That is, the worst error of the expected n-step return is guarante...&hellip;</p>
       <div class="post-list__meta"><time datetime="2017-10-23 13:45:00 +0800" class="post-list__meta--date date">2017-10-23</time><a class="btn-border-small" href=/2017/10/Multi-Step-Bootstrapping/>继续阅读</a></div>
   </div>
   
</section>

<section class="post-comments">
  
    <div id="disqus_thread"></div>
    <script>
    
    var disqus_config = function () {
        this.page.url = "http://kornbergfresnel.github.io/2017/10/Monte-Carlo-Methods/";
        this.page.identifier = "/2017/10/Monte-Carlo-Methods/";
    };

    var disqus_shortname = 'vno-jekyll';
    
    (function() { // DON'T EDIT BELOW THIS LINE
        var d = document, s = d.createElement('script');
        s.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();
    </script>
    <noscript>要查看<a href="http://disqus.com/?ref_noscript"> Disqus </a>评论，请启用 JavaScript</noscript>
    
  
  
  
  
</section>


            <section class="footer">
    <footer>
    	<span class="footer__copyright">本站点采用<a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">知识共享 署名-非商业性使用-相同方式共享 4.0 国际 许可协议</a></span>
        <span class="footer__copyright">由 <a href="https://jekyllrb.com">Jekyll</a> 于 2018-01-06 生成，感谢 <a href="https://www.digitalocean.com/?refcode=30ed2d146762">Digital Ocean</a> 为本站提供稳定的 VPS 服务</span>
        <span class="footer__copyright">本站由 <a href="http://twitter/andrew_kornberg">@andrew_kornberg</a> 创建，采用 <a href="https://github.com/onevcat/vno-jekyll">Vno - Jekyll</a> 作为主题，您可以在 GitHub 找到<a href="https://github.com/onevcat/OneV-s-Den">本站源码</a> - &copy; 2018</span>
    </footer>
</section>

        </div>
    </div>
    
    <script type="text/javascript" src="//code.jquery.com/jquery-1.11.3.min.js"></script>

<script type="text/javascript" src="/js/main.js"></script>



    
  </body>

</html>
